{"text": " Hi, and welcome back to this channel. In this video, we're going to be looking at the new chat GPT feature that allows for you to create custom GPTs. When you load up chat.openai.com, your new window looks something like this. On the left-hand side, you'll see this Explore button. If we click Explore, we'll be taken to a new prompt where we can go ahead and start creating a new GPT. In this video, we're going to try to create a new GPT that can classify specific kinds of entities, namely vehicles and buildings. To do this, we're going to hit Create a GPT and we'll be taken to this prompt here. This page allows for us to pass natural language instructions to create a set of custom rules for this GPT model to follow. What's really cool about this is that a OpenAI will take care of creating everything for us automatically with natural language. So what do we want this GPT to really do? Well, what we wanted to do is we wanted to be able to take in an input text and automatically annotate it by giving us things like different kinds of buildings, people, places, etc. In other words, we wanted to function kind of like a named entity recognizer that can work without a scope data. Let's go ahead and give it these very natural instructions. Create a GPT that can identify named entities. I want to find things like vehicles, places, people, events, dates, and buildings. Output the data as JSON only. And this will let us have a JSON schema that we can use going forward. So let's go ahead and just hit Enter. And we'll notice that immediately OpenAI's GPT builder starts spinning up. Now, what's happening here in the back end is OpenAI is taking our natural instructions and translating them into proper configuration rules, essentially, which was he populated in the Configure section in just a moment. And as you can see, this entire process only takes roughly about 20 seconds to do. Once this is done, it's going to prompt us to give it a new name. And we can go ahead and just accept what it gives us. Yes, I like that. So go ahead and name it essentially entity explorer. And we'll see that it's happy with our choice. And then it's finally going to prompt for us to generate essentially an image for this. This will be its profile image. And it's using Dolly 3 to do this in just a moment. This is going to be done. While this is happening, let's go ahead and explore what's happening on the right hand of the screen. On the right side of the screen, we see things like find entities in this text and all these other little prompts. This is the result of the config file that's already been generated for us. And this is where we can test out how this model is working and if it's doing what we want it to do. And we're going to go ahead and say, yes, keep that. And we're happy with it. So now that we have our image, our title, and our set of instructions, let's go ahead and take a look at the configure section. As you can tell, we've gone ahead and already got this all populated for us. This is what GPT builder was doing in the background. As you can tell, we've got a couple different things here, such as the name of it, which hasn't been updated yet, a description of it, which we can see populated here, and a brief set of instructions, which it's translated for us into probably a better collection of instructions. And it's also given us a couple conversation starters, which pop up right here. Let's go ahead and go back to create and take a look at what we can do. So let's go ahead and say, find entities in this text. But I'm just going to go ahead and paste in an opening bit of dialogue from a United States Holocaust Memorial Museum testimony. And as you can tell, all I've had to do in this is in real time is simply paste in a single block of text. Now what I like to do when I'm creating one of these builders is I like to give it as few instructions as possible, especially if I'm doing something like classification, which named entity recognition is. And the reason for this is because it will come up with a schema. And I can look at the schema and say, hmm, do I like this or do I not like this? And what I can do for my next iteration of this creation, when I start to adjust these instructions, is I can use the schema that it's generated and go ahead and accept that and apply it into the main text. So as you can see, we're getting a couple outputs and we have different kinds of entities being populated. We have a person in this case, Agnes Adaki, and it keeps on going on down the list. And it even has extracted this individual's maiden name as well. If we keep on going down, we see other kinds of events. In this case, this is a place. And it's also specifically referencing Budapest Hungry and the date of the event actually taking place. So in this case, this isn't functioning like a typical NER model. Rather, it's giving a lot of extra metadata. What's really nice about this is for as far as I can tell, I'm not seeing any hallucinations. Now that's not to indicate that hallucinations won't actually occur. In fact, you can probably expect that they do. What I like to do though is just use this as a way to kind of explore my data real quickly and do some manual validation. This helps me think through ways in which I can kind of structure structured metadata. But let's just pretend for just a moment that I'd like all of this. What can I do at this stage? Well, let's say I want to just work with this internally. I can click the Save button and I can select, I only want to be the one to view this and work with it. I'll do this when I'm developing this, at least testing out a couple things. When I'm ready to send this out and have other individuals test this model and how it's performing on out of scope data that probably I can't come up with, I can then allow only people with the link to actually access and use this model. And then finally, once I pass that test, I can go ahead and select public and make this publicly available for others to test it and give larger feedback. But overall, that's a very quick introduction to this new OpenAI feature", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.46, "text": " Hi, and welcome back to this channel.", "tokens": [50364, 2421, 11, 293, 2928, 646, 281, 341, 2269, 13, 50487], "temperature": 0.0, "avg_logprob": -0.15954627990722656, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3021550178527832}, {"id": 1, "seek": 0, "start": 2.46, "end": 3.8000000000000003, "text": " In this video, we're going to be looking at", "tokens": [50487, 682, 341, 960, 11, 321, 434, 516, 281, 312, 1237, 412, 50554], "temperature": 0.0, "avg_logprob": -0.15954627990722656, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3021550178527832}, {"id": 2, "seek": 0, "start": 3.8000000000000003, "end": 8.0, "text": " the new chat GPT feature that allows for you to create custom GPTs.", "tokens": [50554, 264, 777, 5081, 26039, 51, 4111, 300, 4045, 337, 291, 281, 1884, 2375, 26039, 33424, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15954627990722656, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3021550178527832}, {"id": 3, "seek": 0, "start": 8.0, "end": 10.32, "text": " When you load up chat.openai.com,", "tokens": [50764, 1133, 291, 3677, 493, 5081, 13, 15752, 1301, 13, 1112, 11, 50880], "temperature": 0.0, "avg_logprob": -0.15954627990722656, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3021550178527832}, {"id": 4, "seek": 0, "start": 10.32, "end": 12.52, "text": " your new window looks something like this.", "tokens": [50880, 428, 777, 4910, 1542, 746, 411, 341, 13, 50990], "temperature": 0.0, "avg_logprob": -0.15954627990722656, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3021550178527832}, {"id": 5, "seek": 0, "start": 12.52, "end": 15.16, "text": " On the left-hand side, you'll see this Explore button.", "tokens": [50990, 1282, 264, 1411, 12, 5543, 1252, 11, 291, 603, 536, 341, 12514, 418, 2960, 13, 51122], "temperature": 0.0, "avg_logprob": -0.15954627990722656, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3021550178527832}, {"id": 6, "seek": 0, "start": 15.16, "end": 20.52, "text": " If we click Explore, we'll be taken to a new prompt where we can go ahead and start creating a new GPT.", "tokens": [51122, 759, 321, 2052, 12514, 418, 11, 321, 603, 312, 2726, 281, 257, 777, 12391, 689, 321, 393, 352, 2286, 293, 722, 4084, 257, 777, 26039, 51, 13, 51390], "temperature": 0.0, "avg_logprob": -0.15954627990722656, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3021550178527832}, {"id": 7, "seek": 0, "start": 20.52, "end": 23.92, "text": " In this video, we're going to try to create a new GPT that can classify", "tokens": [51390, 682, 341, 960, 11, 321, 434, 516, 281, 853, 281, 1884, 257, 777, 26039, 51, 300, 393, 33872, 51560], "temperature": 0.0, "avg_logprob": -0.15954627990722656, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3021550178527832}, {"id": 8, "seek": 0, "start": 23.92, "end": 27.6, "text": " specific kinds of entities, namely vehicles and buildings.", "tokens": [51560, 2685, 3685, 295, 16667, 11, 20926, 8948, 293, 7446, 13, 51744], "temperature": 0.0, "avg_logprob": -0.15954627990722656, "compression_ratio": 1.7491525423728813, "no_speech_prob": 0.3021550178527832}, {"id": 9, "seek": 2760, "start": 27.6, "end": 32.36, "text": " To do this, we're going to hit Create a GPT and we'll be taken to this prompt here.", "tokens": [50364, 1407, 360, 341, 11, 321, 434, 516, 281, 2045, 20248, 257, 26039, 51, 293, 321, 603, 312, 2726, 281, 341, 12391, 510, 13, 50602], "temperature": 0.0, "avg_logprob": -0.17368939814676765, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.0038144143763929605}, {"id": 10, "seek": 2760, "start": 32.36, "end": 37.84, "text": " This page allows for us to pass natural language instructions to create a set of", "tokens": [50602, 639, 3028, 4045, 337, 505, 281, 1320, 3303, 2856, 9415, 281, 1884, 257, 992, 295, 50876], "temperature": 0.0, "avg_logprob": -0.17368939814676765, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.0038144143763929605}, {"id": 11, "seek": 2760, "start": 37.84, "end": 41.120000000000005, "text": " custom rules for this GPT model to follow.", "tokens": [50876, 2375, 4474, 337, 341, 26039, 51, 2316, 281, 1524, 13, 51040], "temperature": 0.0, "avg_logprob": -0.17368939814676765, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.0038144143763929605}, {"id": 12, "seek": 2760, "start": 41.120000000000005, "end": 42.92, "text": " What's really cool about this is that", "tokens": [51040, 708, 311, 534, 1627, 466, 341, 307, 300, 51130], "temperature": 0.0, "avg_logprob": -0.17368939814676765, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.0038144143763929605}, {"id": 13, "seek": 2760, "start": 42.92, "end": 48.08, "text": " a OpenAI will take care of creating everything for us automatically with natural language.", "tokens": [51130, 257, 7238, 48698, 486, 747, 1127, 295, 4084, 1203, 337, 505, 6772, 365, 3303, 2856, 13, 51388], "temperature": 0.0, "avg_logprob": -0.17368939814676765, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.0038144143763929605}, {"id": 14, "seek": 2760, "start": 48.08, "end": 51.120000000000005, "text": " So what do we want this GPT to really do?", "tokens": [51388, 407, 437, 360, 321, 528, 341, 26039, 51, 281, 534, 360, 30, 51540], "temperature": 0.0, "avg_logprob": -0.17368939814676765, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.0038144143763929605}, {"id": 15, "seek": 2760, "start": 51.120000000000005, "end": 53.160000000000004, "text": " Well, what we wanted to do is we wanted to be able to take in", "tokens": [51540, 1042, 11, 437, 321, 1415, 281, 360, 307, 321, 1415, 281, 312, 1075, 281, 747, 294, 51642], "temperature": 0.0, "avg_logprob": -0.17368939814676765, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.0038144143763929605}, {"id": 16, "seek": 2760, "start": 53.160000000000004, "end": 57.2, "text": " an input text and automatically annotate it by giving us things like", "tokens": [51642, 364, 4846, 2487, 293, 6772, 25339, 473, 309, 538, 2902, 505, 721, 411, 51844], "temperature": 0.0, "avg_logprob": -0.17368939814676765, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.0038144143763929605}, {"id": 17, "seek": 5720, "start": 57.28, "end": 60.720000000000006, "text": " different kinds of buildings, people, places, etc.", "tokens": [50368, 819, 3685, 295, 7446, 11, 561, 11, 3190, 11, 5183, 13, 50540], "temperature": 0.0, "avg_logprob": -0.14933604134453668, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0024321035016328096}, {"id": 18, "seek": 5720, "start": 60.720000000000006, "end": 64.76, "text": " In other words, we wanted to function kind of like a named entity recognizer", "tokens": [50540, 682, 661, 2283, 11, 321, 1415, 281, 2445, 733, 295, 411, 257, 4926, 13977, 3068, 6545, 50742], "temperature": 0.0, "avg_logprob": -0.14933604134453668, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0024321035016328096}, {"id": 19, "seek": 5720, "start": 64.76, "end": 67.12, "text": " that can work without a scope data.", "tokens": [50742, 300, 393, 589, 1553, 257, 11923, 1412, 13, 50860], "temperature": 0.0, "avg_logprob": -0.14933604134453668, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0024321035016328096}, {"id": 20, "seek": 5720, "start": 67.12, "end": 70.4, "text": " Let's go ahead and give it these very natural instructions.", "tokens": [50860, 961, 311, 352, 2286, 293, 976, 309, 613, 588, 3303, 9415, 13, 51024], "temperature": 0.0, "avg_logprob": -0.14933604134453668, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0024321035016328096}, {"id": 21, "seek": 5720, "start": 70.4, "end": 76.92, "text": " Create a GPT that can identify named entities.", "tokens": [51024, 20248, 257, 26039, 51, 300, 393, 5876, 4926, 16667, 13, 51350], "temperature": 0.0, "avg_logprob": -0.14933604134453668, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0024321035016328096}, {"id": 22, "seek": 5720, "start": 76.92, "end": 87.16, "text": " I want to find things like vehicles, places, people, events, dates, and buildings.", "tokens": [51350, 286, 528, 281, 915, 721, 411, 8948, 11, 3190, 11, 561, 11, 3931, 11, 11691, 11, 293, 7446, 13, 51862], "temperature": 0.0, "avg_logprob": -0.14933604134453668, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.0024321035016328096}, {"id": 23, "seek": 8716, "start": 87.36, "end": 94.0, "text": " Output the data as JSON only.", "tokens": [50374, 5925, 2582, 264, 1412, 382, 31828, 787, 13, 50706], "temperature": 0.0, "avg_logprob": -0.1582408676976743, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0002424551930744201}, {"id": 24, "seek": 8716, "start": 94.0, "end": 98.08, "text": " And this will let us have a JSON schema that we can use going forward.", "tokens": [50706, 400, 341, 486, 718, 505, 362, 257, 31828, 34078, 300, 321, 393, 764, 516, 2128, 13, 50910], "temperature": 0.0, "avg_logprob": -0.1582408676976743, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0002424551930744201}, {"id": 25, "seek": 8716, "start": 98.08, "end": 99.8, "text": " So let's go ahead and just hit Enter.", "tokens": [50910, 407, 718, 311, 352, 2286, 293, 445, 2045, 10399, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1582408676976743, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0002424551930744201}, {"id": 26, "seek": 8716, "start": 99.8, "end": 104.47999999999999, "text": " And we'll notice that immediately OpenAI's GPT builder starts spinning up.", "tokens": [50996, 400, 321, 603, 3449, 300, 4258, 7238, 48698, 311, 26039, 51, 27377, 3719, 15640, 493, 13, 51230], "temperature": 0.0, "avg_logprob": -0.1582408676976743, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0002424551930744201}, {"id": 27, "seek": 8716, "start": 104.47999999999999, "end": 108.08, "text": " Now, what's happening here in the back end is OpenAI is taking our natural", "tokens": [51230, 823, 11, 437, 311, 2737, 510, 294, 264, 646, 917, 307, 7238, 48698, 307, 1940, 527, 3303, 51410], "temperature": 0.0, "avg_logprob": -0.1582408676976743, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0002424551930744201}, {"id": 28, "seek": 8716, "start": 108.08, "end": 113.88, "text": " instructions and translating them into proper configuration rules, essentially,", "tokens": [51410, 9415, 293, 35030, 552, 666, 2296, 11694, 4474, 11, 4476, 11, 51700], "temperature": 0.0, "avg_logprob": -0.1582408676976743, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0002424551930744201}, {"id": 29, "seek": 11388, "start": 113.88, "end": 117.08, "text": " which was he populated in the Configure section in just a moment.", "tokens": [50364, 597, 390, 415, 32998, 294, 264, 44151, 540, 3541, 294, 445, 257, 1623, 13, 50524], "temperature": 0.0, "avg_logprob": -0.1430219201480641, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.05867908149957657}, {"id": 30, "seek": 11388, "start": 117.08, "end": 123.03999999999999, "text": " And as you can see, this entire process only takes roughly about 20 seconds to do.", "tokens": [50524, 400, 382, 291, 393, 536, 11, 341, 2302, 1399, 787, 2516, 9810, 466, 945, 3949, 281, 360, 13, 50822], "temperature": 0.0, "avg_logprob": -0.1430219201480641, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.05867908149957657}, {"id": 31, "seek": 11388, "start": 123.03999999999999, "end": 126.67999999999999, "text": " Once this is done, it's going to prompt us to give it a new name.", "tokens": [50822, 3443, 341, 307, 1096, 11, 309, 311, 516, 281, 12391, 505, 281, 976, 309, 257, 777, 1315, 13, 51004], "temperature": 0.0, "avg_logprob": -0.1430219201480641, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.05867908149957657}, {"id": 32, "seek": 11388, "start": 126.67999999999999, "end": 129.44, "text": " And we can go ahead and just accept what it gives us.", "tokens": [51004, 400, 321, 393, 352, 2286, 293, 445, 3241, 437, 309, 2709, 505, 13, 51142], "temperature": 0.0, "avg_logprob": -0.1430219201480641, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.05867908149957657}, {"id": 33, "seek": 11388, "start": 129.44, "end": 130.72, "text": " Yes, I like that.", "tokens": [51142, 1079, 11, 286, 411, 300, 13, 51206], "temperature": 0.0, "avg_logprob": -0.1430219201480641, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.05867908149957657}, {"id": 34, "seek": 11388, "start": 130.72, "end": 134.68, "text": " So go ahead and name it essentially entity explorer.", "tokens": [51206, 407, 352, 2286, 293, 1315, 309, 4476, 13977, 39680, 13, 51404], "temperature": 0.0, "avg_logprob": -0.1430219201480641, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.05867908149957657}, {"id": 35, "seek": 11388, "start": 134.68, "end": 136.96, "text": " And we'll see that it's happy with our choice.", "tokens": [51404, 400, 321, 603, 536, 300, 309, 311, 2055, 365, 527, 3922, 13, 51518], "temperature": 0.0, "avg_logprob": -0.1430219201480641, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.05867908149957657}, {"id": 36, "seek": 11388, "start": 136.96, "end": 140.84, "text": " And then it's finally going to prompt for us to generate essentially an image for this.", "tokens": [51518, 400, 550, 309, 311, 2721, 516, 281, 12391, 337, 505, 281, 8460, 4476, 364, 3256, 337, 341, 13, 51712], "temperature": 0.0, "avg_logprob": -0.1430219201480641, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.05867908149957657}, {"id": 37, "seek": 11388, "start": 140.84, "end": 142.88, "text": " This will be its profile image.", "tokens": [51712, 639, 486, 312, 1080, 7964, 3256, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1430219201480641, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.05867908149957657}, {"id": 38, "seek": 14288, "start": 142.92, "end": 145.6, "text": " And it's using Dolly 3 to do this in just a moment.", "tokens": [50366, 400, 309, 311, 1228, 1144, 13020, 805, 281, 360, 341, 294, 445, 257, 1623, 13, 50500], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 39, "seek": 14288, "start": 145.6, "end": 147.32, "text": " This is going to be done.", "tokens": [50500, 639, 307, 516, 281, 312, 1096, 13, 50586], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 40, "seek": 14288, "start": 147.32, "end": 150.48, "text": " While this is happening, let's go ahead and explore what's happening on the right", "tokens": [50586, 3987, 341, 307, 2737, 11, 718, 311, 352, 2286, 293, 6839, 437, 311, 2737, 322, 264, 558, 50744], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 41, "seek": 14288, "start": 150.48, "end": 151.72, "text": " hand of the screen.", "tokens": [50744, 1011, 295, 264, 2568, 13, 50806], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 42, "seek": 14288, "start": 151.72, "end": 155.44, "text": " On the right side of the screen, we see things like find entities in this text and", "tokens": [50806, 1282, 264, 558, 1252, 295, 264, 2568, 11, 321, 536, 721, 411, 915, 16667, 294, 341, 2487, 293, 50992], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 43, "seek": 14288, "start": 155.44, "end": 156.96, "text": " all these other little prompts.", "tokens": [50992, 439, 613, 661, 707, 41095, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 44, "seek": 14288, "start": 156.96, "end": 160.92, "text": " This is the result of the config file that's already been generated for us.", "tokens": [51068, 639, 307, 264, 1874, 295, 264, 6662, 3991, 300, 311, 1217, 668, 10833, 337, 505, 13, 51266], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 45, "seek": 14288, "start": 160.92, "end": 164.07999999999998, "text": " And this is where we can test out how this model is working and", "tokens": [51266, 400, 341, 307, 689, 321, 393, 1500, 484, 577, 341, 2316, 307, 1364, 293, 51424], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 46, "seek": 14288, "start": 164.07999999999998, "end": 166.16, "text": " if it's doing what we want it to do.", "tokens": [51424, 498, 309, 311, 884, 437, 321, 528, 309, 281, 360, 13, 51528], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 47, "seek": 14288, "start": 166.16, "end": 168.96, "text": " And we're going to go ahead and say, yes, keep that.", "tokens": [51528, 400, 321, 434, 516, 281, 352, 2286, 293, 584, 11, 2086, 11, 1066, 300, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 48, "seek": 14288, "start": 168.96, "end": 170.28, "text": " And we're happy with it.", "tokens": [51668, 400, 321, 434, 2055, 365, 309, 13, 51734], "temperature": 0.0, "avg_logprob": -0.1322065661030431, "compression_ratio": 1.8361204013377928, "no_speech_prob": 0.003141002729535103}, {"id": 49, "seek": 17028, "start": 170.28, "end": 173.6, "text": " So now that we have our image, our title, and our set of instructions,", "tokens": [50364, 407, 586, 300, 321, 362, 527, 3256, 11, 527, 4876, 11, 293, 527, 992, 295, 9415, 11, 50530], "temperature": 0.0, "avg_logprob": -0.12291002616607885, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.0004096685443073511}, {"id": 50, "seek": 17028, "start": 173.6, "end": 176.04, "text": " let's go ahead and take a look at the configure section.", "tokens": [50530, 718, 311, 352, 2286, 293, 747, 257, 574, 412, 264, 22162, 3541, 13, 50652], "temperature": 0.0, "avg_logprob": -0.12291002616607885, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.0004096685443073511}, {"id": 51, "seek": 17028, "start": 177.44, "end": 181.6, "text": " As you can tell, we've gone ahead and already got this all populated for us.", "tokens": [50722, 1018, 291, 393, 980, 11, 321, 600, 2780, 2286, 293, 1217, 658, 341, 439, 32998, 337, 505, 13, 50930], "temperature": 0.0, "avg_logprob": -0.12291002616607885, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.0004096685443073511}, {"id": 52, "seek": 17028, "start": 181.6, "end": 184.88, "text": " This is what GPT builder was doing in the background.", "tokens": [50930, 639, 307, 437, 26039, 51, 27377, 390, 884, 294, 264, 3678, 13, 51094], "temperature": 0.0, "avg_logprob": -0.12291002616607885, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.0004096685443073511}, {"id": 53, "seek": 17028, "start": 184.88, "end": 186.84, "text": " As you can tell, we've got a couple different things here,", "tokens": [51094, 1018, 291, 393, 980, 11, 321, 600, 658, 257, 1916, 819, 721, 510, 11, 51192], "temperature": 0.0, "avg_logprob": -0.12291002616607885, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.0004096685443073511}, {"id": 54, "seek": 17028, "start": 186.84, "end": 190.12, "text": " such as the name of it, which hasn't been updated yet, a description of it,", "tokens": [51192, 1270, 382, 264, 1315, 295, 309, 11, 597, 6132, 380, 668, 10588, 1939, 11, 257, 3855, 295, 309, 11, 51356], "temperature": 0.0, "avg_logprob": -0.12291002616607885, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.0004096685443073511}, {"id": 55, "seek": 17028, "start": 190.12, "end": 193.48, "text": " which we can see populated here, and a brief set of instructions,", "tokens": [51356, 597, 321, 393, 536, 32998, 510, 11, 293, 257, 5353, 992, 295, 9415, 11, 51524], "temperature": 0.0, "avg_logprob": -0.12291002616607885, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.0004096685443073511}, {"id": 56, "seek": 17028, "start": 193.48, "end": 197.76, "text": " which it's translated for us into probably a better collection of instructions.", "tokens": [51524, 597, 309, 311, 16805, 337, 505, 666, 1391, 257, 1101, 5765, 295, 9415, 13, 51738], "temperature": 0.0, "avg_logprob": -0.12291002616607885, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.0004096685443073511}, {"id": 57, "seek": 19776, "start": 197.76, "end": 202.44, "text": " And it's also given us a couple conversation starters, which pop up right here.", "tokens": [50364, 400, 309, 311, 611, 2212, 505, 257, 1916, 3761, 35131, 11, 597, 1665, 493, 558, 510, 13, 50598], "temperature": 0.0, "avg_logprob": -0.1449637004307338, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.00045039044925943017}, {"id": 58, "seek": 19776, "start": 202.44, "end": 206.12, "text": " Let's go ahead and go back to create and take a look at what we can do.", "tokens": [50598, 961, 311, 352, 2286, 293, 352, 646, 281, 1884, 293, 747, 257, 574, 412, 437, 321, 393, 360, 13, 50782], "temperature": 0.0, "avg_logprob": -0.1449637004307338, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.00045039044925943017}, {"id": 59, "seek": 19776, "start": 206.12, "end": 209.2, "text": " So let's go ahead and say, find entities in this text.", "tokens": [50782, 407, 718, 311, 352, 2286, 293, 584, 11, 915, 16667, 294, 341, 2487, 13, 50936], "temperature": 0.0, "avg_logprob": -0.1449637004307338, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.00045039044925943017}, {"id": 60, "seek": 19776, "start": 209.2, "end": 213.23999999999998, "text": " But I'm just going to go ahead and paste in an opening bit of dialogue from", "tokens": [50936, 583, 286, 478, 445, 516, 281, 352, 2286, 293, 9163, 294, 364, 5193, 857, 295, 10221, 490, 51138], "temperature": 0.0, "avg_logprob": -0.1449637004307338, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.00045039044925943017}, {"id": 61, "seek": 19776, "start": 213.23999999999998, "end": 215.76, "text": " a United States Holocaust Memorial Museum testimony.", "tokens": [51138, 257, 2824, 3040, 28399, 24957, 10967, 15634, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1449637004307338, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.00045039044925943017}, {"id": 62, "seek": 19776, "start": 217.2, "end": 219.95999999999998, "text": " And as you can tell, all I've had to do in this is in real time is simply", "tokens": [51336, 400, 382, 291, 393, 980, 11, 439, 286, 600, 632, 281, 360, 294, 341, 307, 294, 957, 565, 307, 2935, 51474], "temperature": 0.0, "avg_logprob": -0.1449637004307338, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.00045039044925943017}, {"id": 63, "seek": 19776, "start": 219.95999999999998, "end": 222.95999999999998, "text": " paste in a single block of text.", "tokens": [51474, 9163, 294, 257, 2167, 3461, 295, 2487, 13, 51624], "temperature": 0.0, "avg_logprob": -0.1449637004307338, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.00045039044925943017}, {"id": 64, "seek": 19776, "start": 222.95999999999998, "end": 225.79999999999998, "text": " Now what I like to do when I'm creating one of these builders is I like to give", "tokens": [51624, 823, 437, 286, 411, 281, 360, 562, 286, 478, 4084, 472, 295, 613, 36281, 307, 286, 411, 281, 976, 51766], "temperature": 0.0, "avg_logprob": -0.1449637004307338, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.00045039044925943017}, {"id": 65, "seek": 22580, "start": 225.8, "end": 228.76000000000002, "text": " it as few instructions as possible, especially if I'm doing something like", "tokens": [50364, 309, 382, 1326, 9415, 382, 1944, 11, 2318, 498, 286, 478, 884, 746, 411, 50512], "temperature": 0.0, "avg_logprob": -0.10782934694874044, "compression_ratio": 1.770392749244713, "no_speech_prob": 0.014238731004297733}, {"id": 66, "seek": 22580, "start": 228.76000000000002, "end": 231.68, "text": " classification, which named entity recognition is.", "tokens": [50512, 21538, 11, 597, 4926, 13977, 11150, 307, 13, 50658], "temperature": 0.0, "avg_logprob": -0.10782934694874044, "compression_ratio": 1.770392749244713, "no_speech_prob": 0.014238731004297733}, {"id": 67, "seek": 22580, "start": 231.68, "end": 234.68, "text": " And the reason for this is because it will come up with a schema.", "tokens": [50658, 400, 264, 1778, 337, 341, 307, 570, 309, 486, 808, 493, 365, 257, 34078, 13, 50808], "temperature": 0.0, "avg_logprob": -0.10782934694874044, "compression_ratio": 1.770392749244713, "no_speech_prob": 0.014238731004297733}, {"id": 68, "seek": 22580, "start": 234.68, "end": 239.0, "text": " And I can look at the schema and say, hmm, do I like this or do I not like this?", "tokens": [50808, 400, 286, 393, 574, 412, 264, 34078, 293, 584, 11, 16478, 11, 360, 286, 411, 341, 420, 360, 286, 406, 411, 341, 30, 51024], "temperature": 0.0, "avg_logprob": -0.10782934694874044, "compression_ratio": 1.770392749244713, "no_speech_prob": 0.014238731004297733}, {"id": 69, "seek": 22580, "start": 239.0, "end": 242.16000000000003, "text": " And what I can do for my next iteration of this creation,", "tokens": [51024, 400, 437, 286, 393, 360, 337, 452, 958, 24784, 295, 341, 8016, 11, 51182], "temperature": 0.0, "avg_logprob": -0.10782934694874044, "compression_ratio": 1.770392749244713, "no_speech_prob": 0.014238731004297733}, {"id": 70, "seek": 22580, "start": 242.16000000000003, "end": 247.0, "text": " when I start to adjust these instructions, is I can use the schema that it's generated", "tokens": [51182, 562, 286, 722, 281, 4369, 613, 9415, 11, 307, 286, 393, 764, 264, 34078, 300, 309, 311, 10833, 51424], "temperature": 0.0, "avg_logprob": -0.10782934694874044, "compression_ratio": 1.770392749244713, "no_speech_prob": 0.014238731004297733}, {"id": 71, "seek": 22580, "start": 247.0, "end": 250.48000000000002, "text": " and go ahead and accept that and apply it into the main text.", "tokens": [51424, 293, 352, 2286, 293, 3241, 300, 293, 3079, 309, 666, 264, 2135, 2487, 13, 51598], "temperature": 0.0, "avg_logprob": -0.10782934694874044, "compression_ratio": 1.770392749244713, "no_speech_prob": 0.014238731004297733}, {"id": 72, "seek": 22580, "start": 250.48000000000002, "end": 252.56, "text": " So as you can see, we're getting a couple outputs and", "tokens": [51598, 407, 382, 291, 393, 536, 11, 321, 434, 1242, 257, 1916, 23930, 293, 51702], "temperature": 0.0, "avg_logprob": -0.10782934694874044, "compression_ratio": 1.770392749244713, "no_speech_prob": 0.014238731004297733}, {"id": 73, "seek": 22580, "start": 252.56, "end": 255.16000000000003, "text": " we have different kinds of entities being populated.", "tokens": [51702, 321, 362, 819, 3685, 295, 16667, 885, 32998, 13, 51832], "temperature": 0.0, "avg_logprob": -0.10782934694874044, "compression_ratio": 1.770392749244713, "no_speech_prob": 0.014238731004297733}, {"id": 74, "seek": 25516, "start": 255.16, "end": 260.36, "text": " We have a person in this case, Agnes Adaki, and it keeps on going on down the list.", "tokens": [50364, 492, 362, 257, 954, 294, 341, 1389, 11, 2725, 4081, 1999, 7421, 11, 293, 309, 5965, 322, 516, 322, 760, 264, 1329, 13, 50624], "temperature": 0.0, "avg_logprob": -0.16716031625237263, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.00294436770491302}, {"id": 75, "seek": 25516, "start": 260.36, "end": 264.32, "text": " And it even has extracted this individual's maiden name as well.", "tokens": [50624, 400, 309, 754, 575, 34086, 341, 2609, 311, 48515, 1315, 382, 731, 13, 50822], "temperature": 0.0, "avg_logprob": -0.16716031625237263, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.00294436770491302}, {"id": 76, "seek": 25516, "start": 264.32, "end": 267.12, "text": " If we keep on going down, we see other kinds of events.", "tokens": [50822, 759, 321, 1066, 322, 516, 760, 11, 321, 536, 661, 3685, 295, 3931, 13, 50962], "temperature": 0.0, "avg_logprob": -0.16716031625237263, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.00294436770491302}, {"id": 77, "seek": 25516, "start": 267.12, "end": 269.44, "text": " In this case, this is a place.", "tokens": [50962, 682, 341, 1389, 11, 341, 307, 257, 1081, 13, 51078], "temperature": 0.0, "avg_logprob": -0.16716031625237263, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.00294436770491302}, {"id": 78, "seek": 25516, "start": 269.44, "end": 273.28, "text": " And it's also specifically referencing Budapest Hungry and", "tokens": [51078, 400, 309, 311, 611, 4682, 40582, 6384, 569, 377, 15063, 627, 293, 51270], "temperature": 0.0, "avg_logprob": -0.16716031625237263, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.00294436770491302}, {"id": 79, "seek": 25516, "start": 273.28, "end": 275.8, "text": " the date of the event actually taking place.", "tokens": [51270, 264, 4002, 295, 264, 2280, 767, 1940, 1081, 13, 51396], "temperature": 0.0, "avg_logprob": -0.16716031625237263, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.00294436770491302}, {"id": 80, "seek": 25516, "start": 275.8, "end": 279.0, "text": " So in this case, this isn't functioning like a typical NER model.", "tokens": [51396, 407, 294, 341, 1389, 11, 341, 1943, 380, 18483, 411, 257, 7476, 426, 1598, 2316, 13, 51556], "temperature": 0.0, "avg_logprob": -0.16716031625237263, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.00294436770491302}, {"id": 81, "seek": 25516, "start": 279.0, "end": 281.4, "text": " Rather, it's giving a lot of extra metadata.", "tokens": [51556, 16571, 11, 309, 311, 2902, 257, 688, 295, 2857, 26603, 13, 51676], "temperature": 0.0, "avg_logprob": -0.16716031625237263, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.00294436770491302}, {"id": 82, "seek": 25516, "start": 281.4, "end": 283.92, "text": " What's really nice about this is for as far as I can tell,", "tokens": [51676, 708, 311, 534, 1481, 466, 341, 307, 337, 382, 1400, 382, 286, 393, 980, 11, 51802], "temperature": 0.0, "avg_logprob": -0.16716031625237263, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.00294436770491302}, {"id": 83, "seek": 28392, "start": 283.92, "end": 285.88, "text": " I'm not seeing any hallucinations.", "tokens": [50364, 286, 478, 406, 2577, 604, 35212, 10325, 13, 50462], "temperature": 0.0, "avg_logprob": -0.12125287939001013, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.10559789836406708}, {"id": 84, "seek": 28392, "start": 285.88, "end": 289.2, "text": " Now that's not to indicate that hallucinations won't actually occur.", "tokens": [50462, 823, 300, 311, 406, 281, 13330, 300, 35212, 10325, 1582, 380, 767, 5160, 13, 50628], "temperature": 0.0, "avg_logprob": -0.12125287939001013, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.10559789836406708}, {"id": 85, "seek": 28392, "start": 289.2, "end": 291.64000000000004, "text": " In fact, you can probably expect that they do.", "tokens": [50628, 682, 1186, 11, 291, 393, 1391, 2066, 300, 436, 360, 13, 50750], "temperature": 0.0, "avg_logprob": -0.12125287939001013, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.10559789836406708}, {"id": 86, "seek": 28392, "start": 291.64000000000004, "end": 294.76, "text": " What I like to do though is just use this as a way to kind of explore my data", "tokens": [50750, 708, 286, 411, 281, 360, 1673, 307, 445, 764, 341, 382, 257, 636, 281, 733, 295, 6839, 452, 1412, 50906], "temperature": 0.0, "avg_logprob": -0.12125287939001013, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.10559789836406708}, {"id": 87, "seek": 28392, "start": 294.76, "end": 297.44, "text": " real quickly and do some manual validation.", "tokens": [50906, 957, 2661, 293, 360, 512, 9688, 24071, 13, 51040], "temperature": 0.0, "avg_logprob": -0.12125287939001013, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.10559789836406708}, {"id": 88, "seek": 28392, "start": 297.44, "end": 301.8, "text": " This helps me think through ways in which I can kind of structure structured metadata.", "tokens": [51040, 639, 3665, 385, 519, 807, 2098, 294, 597, 286, 393, 733, 295, 3877, 18519, 26603, 13, 51258], "temperature": 0.0, "avg_logprob": -0.12125287939001013, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.10559789836406708}, {"id": 89, "seek": 28392, "start": 301.8, "end": 305.16, "text": " But let's just pretend for just a moment that I'd like all of this.", "tokens": [51258, 583, 718, 311, 445, 11865, 337, 445, 257, 1623, 300, 286, 1116, 411, 439, 295, 341, 13, 51426], "temperature": 0.0, "avg_logprob": -0.12125287939001013, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.10559789836406708}, {"id": 90, "seek": 28392, "start": 305.16, "end": 306.68, "text": " What can I do at this stage?", "tokens": [51426, 708, 393, 286, 360, 412, 341, 3233, 30, 51502], "temperature": 0.0, "avg_logprob": -0.12125287939001013, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.10559789836406708}, {"id": 91, "seek": 28392, "start": 306.68, "end": 309.96000000000004, "text": " Well, let's say I want to just work with this internally.", "tokens": [51502, 1042, 11, 718, 311, 584, 286, 528, 281, 445, 589, 365, 341, 19501, 13, 51666], "temperature": 0.0, "avg_logprob": -0.12125287939001013, "compression_ratio": 1.7019867549668874, "no_speech_prob": 0.10559789836406708}, {"id": 92, "seek": 30996, "start": 309.96, "end": 312.56, "text": " I can click the Save button and I can select,", "tokens": [50364, 286, 393, 2052, 264, 15541, 2960, 293, 286, 393, 3048, 11, 50494], "temperature": 0.0, "avg_logprob": -0.1047128545826879, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.12299957871437073}, {"id": 93, "seek": 30996, "start": 312.56, "end": 315.59999999999997, "text": " I only want to be the one to view this and work with it.", "tokens": [50494, 286, 787, 528, 281, 312, 264, 472, 281, 1910, 341, 293, 589, 365, 309, 13, 50646], "temperature": 0.0, "avg_logprob": -0.1047128545826879, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.12299957871437073}, {"id": 94, "seek": 30996, "start": 315.59999999999997, "end": 318.96, "text": " I'll do this when I'm developing this, at least testing out a couple things.", "tokens": [50646, 286, 603, 360, 341, 562, 286, 478, 6416, 341, 11, 412, 1935, 4997, 484, 257, 1916, 721, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1047128545826879, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.12299957871437073}, {"id": 95, "seek": 30996, "start": 318.96, "end": 321.91999999999996, "text": " When I'm ready to send this out and have other individuals test this model and", "tokens": [50814, 1133, 286, 478, 1919, 281, 2845, 341, 484, 293, 362, 661, 5346, 1500, 341, 2316, 293, 50962], "temperature": 0.0, "avg_logprob": -0.1047128545826879, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.12299957871437073}, {"id": 96, "seek": 30996, "start": 321.91999999999996, "end": 326.28, "text": " how it's performing on out of scope data that probably I can't come up with,", "tokens": [50962, 577, 309, 311, 10205, 322, 484, 295, 11923, 1412, 300, 1391, 286, 393, 380, 808, 493, 365, 11, 51180], "temperature": 0.0, "avg_logprob": -0.1047128545826879, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.12299957871437073}, {"id": 97, "seek": 30996, "start": 326.28, "end": 331.2, "text": " I can then allow only people with the link to actually access and use this model.", "tokens": [51180, 286, 393, 550, 2089, 787, 561, 365, 264, 2113, 281, 767, 2105, 293, 764, 341, 2316, 13, 51426], "temperature": 0.0, "avg_logprob": -0.1047128545826879, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.12299957871437073}, {"id": 98, "seek": 30996, "start": 331.2, "end": 335.32, "text": " And then finally, once I pass that test, I can go ahead and select public and", "tokens": [51426, 400, 550, 2721, 11, 1564, 286, 1320, 300, 1500, 11, 286, 393, 352, 2286, 293, 3048, 1908, 293, 51632], "temperature": 0.0, "avg_logprob": -0.1047128545826879, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.12299957871437073}, {"id": 99, "seek": 30996, "start": 335.32, "end": 339.32, "text": " make this publicly available for others to test it and give larger feedback.", "tokens": [51632, 652, 341, 14843, 2435, 337, 2357, 281, 1500, 309, 293, 976, 4833, 5824, 13, 51832], "temperature": 0.0, "avg_logprob": -0.1047128545826879, "compression_ratio": 1.804416403785489, "no_speech_prob": 0.12299957871437073}, {"id": 100, "seek": 33932, "start": 339.32, "end": 343.68, "text": " But overall, that's a very quick introduction to this new OpenAI feature", "tokens": [50364, 583, 4787, 11, 300, 311, 257, 588, 1702, 9339, 281, 341, 777, 7238, 48698, 4111, 50582], "temperature": 0.0, "avg_logprob": -0.31618735525343156, "compression_ratio": 0.9473684210526315, "no_speech_prob": 0.3274812400341034}], "language": "en"}