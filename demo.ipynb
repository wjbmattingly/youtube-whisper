{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper for Automatic Speech Recognition (ASR)\n",
    "\n",
    "In this notebook, we will learn about Automatic Speech Recognition and how to implement in Python with OpenAI's Whisper Model.\n",
    "\n",
    "## What is Automatic Speech Recognition (ASR)?\n",
    "\n",
    "Automatic Speech Recognition (ASR) is a technology that allows computers to interpret and transcribe human speech into text. It is a field of study that intersects computer science, linguistics, and electrical engineering. The primary goal of ASR systems is to enable efficient and accurate conversion of spoken language into a written format, which can be used for various applications such as voice-enabled user interfaces, dictation software, and automated transcription services.\n",
    "\n",
    "### Core Components of ASR\n",
    "\n",
    "ASR systems typically involve several core components. Firstly, they include an acoustic model, which recognizes the basic sounds in a language (phonemes). Secondly, there's a language model, which predicts the likelihood of a particular sequence of words. These models are often trained on vast datasets of spoken and written language to improve their accuracy. The integration of these models allows the ASR system to interpret a wide range of speech, even in the presence of background noise, different accents, or dialects.\n",
    "\n",
    "### Applications and Uses\n",
    "\n",
    "The applications of ASR are diverse and widespread. In everyday life, ASR is used in voice-activated GPS systems, virtual assistants like Siri or Alexa, and in smartphones for voice-to-text messaging. In the professional sphere, ASR aids in transcribing meetings, lectures, and interviews, making it a valuable tool in fields like journalism, law, and healthcare. Additionally, ASR technology is pivotal in accessibility, providing assistance to those who have difficulties with typing or using traditional computer interfaces, including individuals with disabilities.\n",
    "\n",
    "### Challenges and Development\n",
    "\n",
    "Despite significant advancements, ASR technology still faces challenges. Accurately recognizing speech in noisy environments, understanding heavily accented speech, or deciphering homophones (words that sound the same but have different meanings) are areas where ASR can struggle. Continuous research and development are focused on improving the robustness and accuracy of ASR systems. This involves training on more diverse datasets and employing advanced algorithms like deep learning to enhance the system's ability to understand context and nuance in spoken language.\n",
    "\n",
    "\n",
    "## Introduction to Whisper\n",
    "\n",
    "Whisper is an advanced machine learning model developed by OpenAI, specializing in speech-to-text transcription. It demonstrates remarkable accuracy in transcribing spoken words into written text, which is a significant breakthrough in the field of natural language processing. This technology is particularly relevant for humanists and archivists, as it bridges the gap between oral and written records, ensuring that spoken language, an essential part of human culture and history, is accurately preserved and accessible.\n",
    "\n",
    "## Benefits for Humanists\n",
    "\n",
    "For humanists, Whisper presents a unique opportunity to delve into oral histories, interviews, and cultural narratives that were previously difficult to document or analyze due to the labor-intensive process of transcription. With Whisper's high accuracy, even in handling various accents and dialects, humanists can now transcribe and analyze large volumes of audio recordings more efficiently. This capability enables a deeper and more nuanced understanding of cultural and historical contexts, as it allows for the preservation and study of diverse linguistic nuances and oral traditions that are often lost in written translation.\n",
    "\n",
    "## Application in Archival Work\n",
    "\n",
    "In the field of archival work, Whisper stands out as a transformative tool. Archivists often deal with vast amounts of audio material, ranging from historical speeches to personal memoirs. The traditional process of transcribing these materials is time-consuming and prone to human error. Whisper automates this process with high accuracy, thus significantly reducing the time and resources needed for transcription. This efficiency not only aids in the preservation of historical audio records but also makes them more accessible to researchers and the public, facilitating a broader engagement with historical material.\n",
    "\n",
    "## Future Implications\n",
    "\n",
    "The integration of Whisper into the workflow of humanists and archivists marks a significant step forward in the digital humanities and archival science. By streamlining the transcription process, Whisper not only conserves resources but also opens up new possibilities for research and preservation. It enables a more inclusive approach to historical and cultural documentation, ensuring that the voices of the past are not only heard but also accurately recorded and analyzed for future generations. As technology continues to advance, Whisper stands as a testament to the potential of machine learning in enhancing our understanding and preservation of human history and culture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's a brief guide on how to install OpenAI's Whisper. This guide assumes you have a basic understanding of Python and its package management system, pip.\n",
    "\n",
    "---\n",
    "\n",
    "## Installing OpenAI Whisper\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before installing Whisper, ensure you have the following prerequisites:\n",
    "\n",
    "1. **Python**: Whisper requires Python. If you don't have Python installed, you can download it from [python.org](https://www.python.org/downloads/).\n",
    "\n",
    "2. **pip**: pip is Python's package installer. It's typically included with Python. To check if you have pip installed, run `pip --version` in your command line.\n",
    "\n",
    "3. **Virtual Environment (Optional but recommended)**: It's a good practice to use a virtual environment for Python projects. This keeps your project's dependencies separate from your global Python installation. You can use `venv` (built into Python) or a third-party tool like `virtualenv`.\n",
    "\n",
    "### Installation Steps\n",
    "\n",
    "1. **Open your command line**: This could be Terminal on macOS/Linux or Command Prompt/PowerShell on Windows.\n",
    "\n",
    "2. **(Optional) Create and activate a virtual environment**:\n",
    "   - Create: `python -m venv myenv` (Replace `myenv` with your preferred environment name)\n",
    "   - Activate: \n",
    "     - Windows: `myenv\\Scripts\\activate`\n",
    "     - macOS/Linux: `source myenv/bin/activate`\n",
    "3. Install ffmpeg\n",
    "    To install FFmpeg for using OpenAI's Whisper on different operating systems, follow these instructions:\n",
    "    - **On MacOS**: Use Homebrew by running `brew install ffmpeg` in the terminal.\n",
    "    - **On Windows**: You can use Chocolatey with the command `choco install ffmpeg` or Scoop with `scoop install ffmpeg`.\n",
    "    - **On Ubuntu or Debian Linux**: Execute `sudo apt update && sudo apt install ffmpeg`.\n",
    "    - **On Arch Linux**: Use `sudo pacman -S ffmpeg`.\n",
    "\n",
    "These commands install FFmpeg, a required tool for Whisper, on your respective operating system. For more details and updates, you can visit the [Whisper GitHub page](https://github.com/openai/whisper).\n",
    "\n",
    "4. **Install Whisper**:\n",
    "   - Run `pip install openai-whisper`.\n",
    "\n",
    "5. **Verify Installation**:\n",
    "   - Once the installation is complete, you can verify it by running `whisper --version` in the command line. This should return the version number of Whisper if it's installed correctly.\n",
    "\n",
    "### Post-Installation\n",
    "\n",
    "- After installing Whisper, you can start using it to transcribe audio files. The basic command is `whisper your_audio_file.mp3`, where `your_audio_file.mp3` is the path to the audio file you want to transcribe.\n",
    "\n",
    "- Whisper also offers various options and configurations for transcription, which you can explore through its documentation or by using `whisper --help` in the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Whisper in Python\n",
    "\n",
    "### Step 1: Importing the Whisper Module\n",
    "\n",
    "To begin using Whisper for speech recognition, the first step is to import the Whisper module into your Python script. This is done with a simple import statement. This statement makes the Whisper library available in your script, enabling you to access its functions and models for speech-to-text transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Loading a Whisper Model\n",
    "\n",
    "The `whisper.load_model(\"small\")` line in Python is used to load a specific model size from OpenAI's Whisper for speech recognition tasks. Whisper offers various model sizes, each with different memory requirements and processing speeds. These sizes include `tiny`, `base`, `small`, `medium`, and `large`, with corresponding English-only versions like `tiny.en`, `base.en`, etc. The smaller models like `tiny` and `base` are faster and require less memory, making them suitable for less resource-intensive applications. Larger models, while slower and requiring more memory, provide better accuracy, especially in challenging audio conditions. The choice of model size depends on the specific requirements of your task, such as the desired balance between speed and accuracy, and the computational resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Transcribing Audio with Whisper\n",
    "\n",
    "After loading the Whisper model, the next step is to transcribe audio. This is done using the `transcribe` method of the model.\n",
    "\n",
    "In this line, `model.transcribe` is called with the path to your audio file (`\"test.mp4\"` in this example). The model processes the audio file and returns the transcription result. The `result` variable will contain a dictionary, where the key `text` holds the transcribed text. This process automatically handles the entire workflow of converting speech in your audio file to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/niso/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"test.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Analyze the Result\n",
    "\n",
    "The result of the transcription using Whisper's `transcribe` method is a dictionary containing two key elements: `'text'` and `'segments'`. \n",
    "\n",
    "- The `'text'` key holds the entire transcribed text from the audio file. In your case, it contains a detailed description of a video tutorial about creating custom GPT models, discussing various features and instructions related to the process.\n",
    "\n",
    "- The `'segments'` key is a list of dictionaries, where each dictionary represents a segment of the audio. Each segment includes details like the segment's ID, the start and end times of the segment in the audio, the transcribed text for that segment, and other metadata such as `avg_logprob`, `compression_ratio`, and `no_speech_prob`. This detailed breakdown can be useful for understanding the transcription at a more granular level, especially for long audio files.\n",
    "\n",
    "The dictionary also includes a `'language'` key that indicates the detected language of the audio, which in this case is English (`'en'`). \n",
    "\n",
    "This structured output allows for a comprehensive understanding of the audio content, making it suitable for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" Hi and welcome back to this channel. In this video, we're going to be looking at the new chat GPT feature that allows for you to create custom GPTs. When you load up chat.openai.com, your new window looks something like this. On the left-hand side, you'll see this explore button. If we click explore, we'll be taken to a new prompt where we can go ahead and start creating a new GPT. In this video, we're going to try to create a new GPT that can classify specific kinds of entities, namely vehicles and buildings. To do this, we're going to hit create a GPT and we'll be taken to this prompt here. This page allows for us to pass natural language instructions to create a set of custom rules for this GPT model to follow. What's really cool about this is that a open AI will take care of creating everything for us automatically with natural language. So what do we want this GPT to really do? Well, what we wanted to do is we wanted to be able to take in an input text and automatically annotate it by giving us things like different kinds of buildings, people, places, etc. In other words, we wanted to function kind of like a named entity recognizer that can work without a scope data. Let's go ahead and give it these very natural instructions. Create a GPT that can identify named entities. I want to find things like vehicles, places, people, events, dates and buildings. Output the data as JSON only. And this will let us have a JSON schema that we can use going forward. So let's go ahead and just hit enter and we'll notice that immediately open AI's GPT builder starts spinning up. Now what's happening here in the back end is open AI is taking our natural instructions and translating them into proper configuration rules, essentially, which was he populated in the configure section in just a moment. As you can see, this entire process only takes roughly about 20 seconds to do. Once this is done, it's going to prompt us to give it a new name and we can go ahead and just accept what it gives us. Yes, I like that. So go ahead and name it essentially entity explorer and we'll see that it's happy with our choice. And then it's finally going to prompt for us to generate essentially an image for this. This will be its profile image and it's using Dolly 3 to do this. Just a moment. This is going to be done. While this is happening, let's go ahead and explore what's happening on the right hand of the screen. On the right side of the screen, we see things like find entities in this text and all these other little prompts. This is the result of the config file that's already been generated for us. And this is where we can test out how this model is working and if it's doing what we want it to do. And we're going to go ahead and say, yes, keep that. And we're happy with it. So now that we have our image, our title and our set of instructions, let's go ahead and take a look at the configure section. As you can tell, we've gone ahead and already got this all populated for us. This is what GPT builder was doing in the background. As you can tell, we've got a couple of different things here, such as the name of it, which hasn't been updated yet, a description of it, which we can see populated here, and a brief set of instructions, which it's translated for us into probably a better collection of instructions. It's also given us a couple conversation starters, which pop up right here. Let's go ahead and go back to create and take a look at what we can do. So let's go ahead and say, find entities in this text. But I'm just going to go ahead and paste in an opening bit of dialogue from a United States Holocaust Memorial Museum testimony. And as you can tell, all I've had to do in this is in real time is simply paste in a single block of text. Now, what I like to do when I'm creating one of these builders is I like to give it as few instructions as possible, especially if I'm doing something like classification, which named entity recognition is. And the reason for this is because it will come up with a schema. And I can look at the schema and say, hmm, do I like this or do I not like this? And what I can do for my next iteration of this creation, when I start to adjust these instructions, is I can use the schema that it's generated and go ahead and accept that and apply it into the main text. So as you can see, we're getting a couple outputs and we have different kinds of entities being populated. We have a person in this case, Agnes Adaki, and it keeps on going on down the list. And it even has extracted this individual's maiden name as well. If we keep on going down, we see other kinds of events. In this case, this is a place. And it's also specifically referencing Budapest Hungry and the date of the event actually taking place. So in this case, it isn't functioning like a typical NER model. Rather, it's giving a lot of extra metadata. What's really nice about this is for as far as I can tell, I'm not seeing any hallucinations. Now, that's not to indicate that hallucinations won't actually occur. In fact, you can probably expect that they do. What I like to do though, is just use this as a way to kind of explore my data real quickly and do some manual validation. This helps me think through ways in which I can kind of structure structured metadata. But let's just pretend for just a moment that I'd like all of this. What can I do at this stage? Well, let's say I want to just work with this internally. I can click the Save button and I can select I only want to be the one to view this and work with it. I'll do this when I'm developing this, at least testing out a couple things. When I'm ready to send this out and have other individuals test this model and how it's performing on out of scope data that probably I can't come up with, I can then allow only people with the link to actually access and use this model. And then finally, once I pass that test, I can go ahead and select Public and make this publicly available for others to test it and give larger feedback. But overall, that's a very quick introduction to this new open AI feature\",\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 2.46,\n",
       "   'text': ' Hi and welcome back to this channel.',\n",
       "   'tokens': [50364, 2421, 293, 2928, 646, 281, 341, 2269, 13, 50487],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16967863195082722,\n",
       "   'compression_ratio': 1.757679180887372,\n",
       "   'no_speech_prob': 0.29533353447914124},\n",
       "  {'id': 1,\n",
       "   'seek': 0,\n",
       "   'start': 2.46,\n",
       "   'end': 7.94,\n",
       "   'text': \" In this video, we're going to be looking at the new chat GPT feature that allows for you to create custom GPTs.\",\n",
       "   'tokens': [50487,\n",
       "    682,\n",
       "    341,\n",
       "    960,\n",
       "    11,\n",
       "    321,\n",
       "    434,\n",
       "    516,\n",
       "    281,\n",
       "    312,\n",
       "    1237,\n",
       "    412,\n",
       "    264,\n",
       "    777,\n",
       "    5081,\n",
       "    26039,\n",
       "    51,\n",
       "    4111,\n",
       "    300,\n",
       "    4045,\n",
       "    337,\n",
       "    291,\n",
       "    281,\n",
       "    1884,\n",
       "    2375,\n",
       "    26039,\n",
       "    33424,\n",
       "    13,\n",
       "    50761],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16967863195082722,\n",
       "   'compression_ratio': 1.757679180887372,\n",
       "   'no_speech_prob': 0.29533353447914124},\n",
       "  {'id': 2,\n",
       "   'seek': 0,\n",
       "   'start': 7.94,\n",
       "   'end': 12.48,\n",
       "   'text': ' When you load up chat.openai.com, your new window looks something like this.',\n",
       "   'tokens': [50761,\n",
       "    1133,\n",
       "    291,\n",
       "    3677,\n",
       "    493,\n",
       "    5081,\n",
       "    13,\n",
       "    15752,\n",
       "    1301,\n",
       "    13,\n",
       "    1112,\n",
       "    11,\n",
       "    428,\n",
       "    777,\n",
       "    4910,\n",
       "    1542,\n",
       "    746,\n",
       "    411,\n",
       "    341,\n",
       "    13,\n",
       "    50988],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16967863195082722,\n",
       "   'compression_ratio': 1.757679180887372,\n",
       "   'no_speech_prob': 0.29533353447914124},\n",
       "  {'id': 3,\n",
       "   'seek': 0,\n",
       "   'start': 12.48,\n",
       "   'end': 15.14,\n",
       "   'text': \" On the left-hand side, you'll see this explore button.\",\n",
       "   'tokens': [50988,\n",
       "    1282,\n",
       "    264,\n",
       "    1411,\n",
       "    12,\n",
       "    5543,\n",
       "    1252,\n",
       "    11,\n",
       "    291,\n",
       "    603,\n",
       "    536,\n",
       "    341,\n",
       "    6839,\n",
       "    2960,\n",
       "    13,\n",
       "    51121],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16967863195082722,\n",
       "   'compression_ratio': 1.757679180887372,\n",
       "   'no_speech_prob': 0.29533353447914124},\n",
       "  {'id': 4,\n",
       "   'seek': 0,\n",
       "   'start': 15.14,\n",
       "   'end': 20.48,\n",
       "   'text': \" If we click explore, we'll be taken to a new prompt where we can go ahead and start creating a new GPT.\",\n",
       "   'tokens': [51121,\n",
       "    759,\n",
       "    321,\n",
       "    2052,\n",
       "    6839,\n",
       "    11,\n",
       "    321,\n",
       "    603,\n",
       "    312,\n",
       "    2726,\n",
       "    281,\n",
       "    257,\n",
       "    777,\n",
       "    12391,\n",
       "    689,\n",
       "    321,\n",
       "    393,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    722,\n",
       "    4084,\n",
       "    257,\n",
       "    777,\n",
       "    26039,\n",
       "    51,\n",
       "    13,\n",
       "    51388],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16967863195082722,\n",
       "   'compression_ratio': 1.757679180887372,\n",
       "   'no_speech_prob': 0.29533353447914124},\n",
       "  {'id': 5,\n",
       "   'seek': 0,\n",
       "   'start': 20.48,\n",
       "   'end': 27.580000000000002,\n",
       "   'text': \" In this video, we're going to try to create a new GPT that can classify specific kinds of entities, namely vehicles and buildings.\",\n",
       "   'tokens': [51388,\n",
       "    682,\n",
       "    341,\n",
       "    960,\n",
       "    11,\n",
       "    321,\n",
       "    434,\n",
       "    516,\n",
       "    281,\n",
       "    853,\n",
       "    281,\n",
       "    1884,\n",
       "    257,\n",
       "    777,\n",
       "    26039,\n",
       "    51,\n",
       "    300,\n",
       "    393,\n",
       "    33872,\n",
       "    2685,\n",
       "    3685,\n",
       "    295,\n",
       "    16667,\n",
       "    11,\n",
       "    20926,\n",
       "    8948,\n",
       "    293,\n",
       "    7446,\n",
       "    13,\n",
       "    51743],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16967863195082722,\n",
       "   'compression_ratio': 1.757679180887372,\n",
       "   'no_speech_prob': 0.29533353447914124},\n",
       "  {'id': 6,\n",
       "   'seek': 2758,\n",
       "   'start': 27.659999999999997,\n",
       "   'end': 32.46,\n",
       "   'text': \" To do this, we're going to hit create a GPT and we'll be taken to this prompt here.\",\n",
       "   'tokens': [50368,\n",
       "    1407,\n",
       "    360,\n",
       "    341,\n",
       "    11,\n",
       "    321,\n",
       "    434,\n",
       "    516,\n",
       "    281,\n",
       "    2045,\n",
       "    1884,\n",
       "    257,\n",
       "    26039,\n",
       "    51,\n",
       "    293,\n",
       "    321,\n",
       "    603,\n",
       "    312,\n",
       "    2726,\n",
       "    281,\n",
       "    341,\n",
       "    12391,\n",
       "    510,\n",
       "    13,\n",
       "    50608],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14979051208496094,\n",
       "   'compression_ratio': 1.7708333333333333,\n",
       "   'no_speech_prob': 0.008432656526565552},\n",
       "  {'id': 7,\n",
       "   'seek': 2758,\n",
       "   'start': 32.46,\n",
       "   'end': 41.22,\n",
       "   'text': ' This page allows for us to pass natural language instructions to create a set of custom rules for this GPT model to follow.',\n",
       "   'tokens': [50608,\n",
       "    639,\n",
       "    3028,\n",
       "    4045,\n",
       "    337,\n",
       "    505,\n",
       "    281,\n",
       "    1320,\n",
       "    3303,\n",
       "    2856,\n",
       "    9415,\n",
       "    281,\n",
       "    1884,\n",
       "    257,\n",
       "    992,\n",
       "    295,\n",
       "    2375,\n",
       "    4474,\n",
       "    337,\n",
       "    341,\n",
       "    26039,\n",
       "    51,\n",
       "    2316,\n",
       "    281,\n",
       "    1524,\n",
       "    13,\n",
       "    51046],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14979051208496094,\n",
       "   'compression_ratio': 1.7708333333333333,\n",
       "   'no_speech_prob': 0.008432656526565552},\n",
       "  {'id': 8,\n",
       "   'seek': 2758,\n",
       "   'start': 41.22,\n",
       "   'end': 48.18,\n",
       "   'text': \" What's really cool about this is that a open AI will take care of creating everything for us automatically with natural language.\",\n",
       "   'tokens': [51046,\n",
       "    708,\n",
       "    311,\n",
       "    534,\n",
       "    1627,\n",
       "    466,\n",
       "    341,\n",
       "    307,\n",
       "    300,\n",
       "    257,\n",
       "    1269,\n",
       "    7318,\n",
       "    486,\n",
       "    747,\n",
       "    1127,\n",
       "    295,\n",
       "    4084,\n",
       "    1203,\n",
       "    337,\n",
       "    505,\n",
       "    6772,\n",
       "    365,\n",
       "    3303,\n",
       "    2856,\n",
       "    13,\n",
       "    51394],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14979051208496094,\n",
       "   'compression_ratio': 1.7708333333333333,\n",
       "   'no_speech_prob': 0.008432656526565552},\n",
       "  {'id': 9,\n",
       "   'seek': 2758,\n",
       "   'start': 48.18,\n",
       "   'end': 51.08,\n",
       "   'text': ' So what do we want this GPT to really do?',\n",
       "   'tokens': [51394,\n",
       "    407,\n",
       "    437,\n",
       "    360,\n",
       "    321,\n",
       "    528,\n",
       "    341,\n",
       "    26039,\n",
       "    51,\n",
       "    281,\n",
       "    534,\n",
       "    360,\n",
       "    30,\n",
       "    51539],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14979051208496094,\n",
       "   'compression_ratio': 1.7708333333333333,\n",
       "   'no_speech_prob': 0.008432656526565552},\n",
       "  {'id': 10,\n",
       "   'seek': 2758,\n",
       "   'start': 51.08,\n",
       "   'end': 57.26,\n",
       "   'text': ' Well, what we wanted to do is we wanted to be able to take in an input text and automatically annotate it by giving us things like',\n",
       "   'tokens': [51539,\n",
       "    1042,\n",
       "    11,\n",
       "    437,\n",
       "    321,\n",
       "    1415,\n",
       "    281,\n",
       "    360,\n",
       "    307,\n",
       "    321,\n",
       "    1415,\n",
       "    281,\n",
       "    312,\n",
       "    1075,\n",
       "    281,\n",
       "    747,\n",
       "    294,\n",
       "    364,\n",
       "    4846,\n",
       "    2487,\n",
       "    293,\n",
       "    6772,\n",
       "    25339,\n",
       "    473,\n",
       "    309,\n",
       "    538,\n",
       "    2902,\n",
       "    505,\n",
       "    721,\n",
       "    411,\n",
       "    51848],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14979051208496094,\n",
       "   'compression_ratio': 1.7708333333333333,\n",
       "   'no_speech_prob': 0.008432656526565552},\n",
       "  {'id': 11,\n",
       "   'seek': 5726,\n",
       "   'start': 57.339999999999996,\n",
       "   'end': 60.739999999999995,\n",
       "   'text': ' different kinds of buildings, people, places, etc.',\n",
       "   'tokens': [50368,\n",
       "    819,\n",
       "    3685,\n",
       "    295,\n",
       "    7446,\n",
       "    11,\n",
       "    561,\n",
       "    11,\n",
       "    3190,\n",
       "    11,\n",
       "    5183,\n",
       "    13,\n",
       "    50538],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14954238102353853,\n",
       "   'compression_ratio': 1.5784753363228698,\n",
       "   'no_speech_prob': 0.0008039791136980057},\n",
       "  {'id': 12,\n",
       "   'seek': 5726,\n",
       "   'start': 60.739999999999995,\n",
       "   'end': 67.14,\n",
       "   'text': ' In other words, we wanted to function kind of like a named entity recognizer that can work without a scope data.',\n",
       "   'tokens': [50538,\n",
       "    682,\n",
       "    661,\n",
       "    2283,\n",
       "    11,\n",
       "    321,\n",
       "    1415,\n",
       "    281,\n",
       "    2445,\n",
       "    733,\n",
       "    295,\n",
       "    411,\n",
       "    257,\n",
       "    4926,\n",
       "    13977,\n",
       "    3068,\n",
       "    6545,\n",
       "    300,\n",
       "    393,\n",
       "    589,\n",
       "    1553,\n",
       "    257,\n",
       "    11923,\n",
       "    1412,\n",
       "    13,\n",
       "    50858],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14954238102353853,\n",
       "   'compression_ratio': 1.5784753363228698,\n",
       "   'no_speech_prob': 0.0008039791136980057},\n",
       "  {'id': 13,\n",
       "   'seek': 5726,\n",
       "   'start': 67.14,\n",
       "   'end': 70.44,\n",
       "   'text': \" Let's go ahead and give it these very natural instructions.\",\n",
       "   'tokens': [50858,\n",
       "    961,\n",
       "    311,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    976,\n",
       "    309,\n",
       "    613,\n",
       "    588,\n",
       "    3303,\n",
       "    9415,\n",
       "    13,\n",
       "    51023],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14954238102353853,\n",
       "   'compression_ratio': 1.5784753363228698,\n",
       "   'no_speech_prob': 0.0008039791136980057},\n",
       "  {'id': 14,\n",
       "   'seek': 5726,\n",
       "   'start': 70.44,\n",
       "   'end': 76.94,\n",
       "   'text': ' Create a GPT that can identify named entities.',\n",
       "   'tokens': [51023,\n",
       "    20248,\n",
       "    257,\n",
       "    26039,\n",
       "    51,\n",
       "    300,\n",
       "    393,\n",
       "    5876,\n",
       "    4926,\n",
       "    16667,\n",
       "    13,\n",
       "    51348],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14954238102353853,\n",
       "   'compression_ratio': 1.5784753363228698,\n",
       "   'no_speech_prob': 0.0008039791136980057},\n",
       "  {'id': 15,\n",
       "   'seek': 5726,\n",
       "   'start': 76.94,\n",
       "   'end': 87.24,\n",
       "   'text': ' I want to find things like vehicles, places, people, events, dates and buildings.',\n",
       "   'tokens': [51348,\n",
       "    286,\n",
       "    528,\n",
       "    281,\n",
       "    915,\n",
       "    721,\n",
       "    411,\n",
       "    8948,\n",
       "    11,\n",
       "    3190,\n",
       "    11,\n",
       "    561,\n",
       "    11,\n",
       "    3931,\n",
       "    11,\n",
       "    11691,\n",
       "    293,\n",
       "    7446,\n",
       "    13,\n",
       "    51863],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14954238102353853,\n",
       "   'compression_ratio': 1.5784753363228698,\n",
       "   'no_speech_prob': 0.0008039791136980057},\n",
       "  {'id': 16,\n",
       "   'seek': 8724,\n",
       "   'start': 87.32,\n",
       "   'end': 94.02,\n",
       "   'text': ' Output the data as JSON only.',\n",
       "   'tokens': [50368, 5925, 2582, 264, 1412, 382, 31828, 787, 13, 50703],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19083036290537012,\n",
       "   'compression_ratio': 1.5897435897435896,\n",
       "   'no_speech_prob': 0.0002858589286915958},\n",
       "  {'id': 17,\n",
       "   'seek': 8724,\n",
       "   'start': 94.02,\n",
       "   'end': 98.08,\n",
       "   'text': ' And this will let us have a JSON schema that we can use going forward.',\n",
       "   'tokens': [50703,\n",
       "    400,\n",
       "    341,\n",
       "    486,\n",
       "    718,\n",
       "    505,\n",
       "    362,\n",
       "    257,\n",
       "    31828,\n",
       "    34078,\n",
       "    300,\n",
       "    321,\n",
       "    393,\n",
       "    764,\n",
       "    516,\n",
       "    2128,\n",
       "    13,\n",
       "    50906],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19083036290537012,\n",
       "   'compression_ratio': 1.5897435897435896,\n",
       "   'no_speech_prob': 0.0002858589286915958},\n",
       "  {'id': 18,\n",
       "   'seek': 8724,\n",
       "   'start': 98.08,\n",
       "   'end': 104.44,\n",
       "   'text': \" So let's go ahead and just hit enter and we'll notice that immediately open AI's GPT builder starts spinning up.\",\n",
       "   'tokens': [50906,\n",
       "    407,\n",
       "    718,\n",
       "    311,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    445,\n",
       "    2045,\n",
       "    3242,\n",
       "    293,\n",
       "    321,\n",
       "    603,\n",
       "    3449,\n",
       "    300,\n",
       "    4258,\n",
       "    1269,\n",
       "    7318,\n",
       "    311,\n",
       "    26039,\n",
       "    51,\n",
       "    27377,\n",
       "    3719,\n",
       "    15640,\n",
       "    493,\n",
       "    13,\n",
       "    51224],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19083036290537012,\n",
       "   'compression_ratio': 1.5897435897435896,\n",
       "   'no_speech_prob': 0.0002858589286915958},\n",
       "  {'id': 19,\n",
       "   'seek': 8724,\n",
       "   'start': 104.44,\n",
       "   'end': 113.03999999999999,\n",
       "   'text': \" Now what's happening here in the back end is open AI is taking our natural instructions and translating them into proper configuration rules,\",\n",
       "   'tokens': [51224,\n",
       "    823,\n",
       "    437,\n",
       "    311,\n",
       "    2737,\n",
       "    510,\n",
       "    294,\n",
       "    264,\n",
       "    646,\n",
       "    917,\n",
       "    307,\n",
       "    1269,\n",
       "    7318,\n",
       "    307,\n",
       "    1940,\n",
       "    527,\n",
       "    3303,\n",
       "    9415,\n",
       "    293,\n",
       "    35030,\n",
       "    552,\n",
       "    666,\n",
       "    2296,\n",
       "    11694,\n",
       "    4474,\n",
       "    11,\n",
       "    51654],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19083036290537012,\n",
       "   'compression_ratio': 1.5897435897435896,\n",
       "   'no_speech_prob': 0.0002858589286915958},\n",
       "  {'id': 20,\n",
       "   'seek': 8724,\n",
       "   'start': 113.03999999999999,\n",
       "   'end': 117.17999999999999,\n",
       "   'text': ' essentially, which was he populated in the configure section in just a moment.',\n",
       "   'tokens': [51654,\n",
       "    4476,\n",
       "    11,\n",
       "    597,\n",
       "    390,\n",
       "    415,\n",
       "    32998,\n",
       "    294,\n",
       "    264,\n",
       "    22162,\n",
       "    3541,\n",
       "    294,\n",
       "    445,\n",
       "    257,\n",
       "    1623,\n",
       "    13,\n",
       "    51861],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19083036290537012,\n",
       "   'compression_ratio': 1.5897435897435896,\n",
       "   'no_speech_prob': 0.0002858589286915958},\n",
       "  {'id': 21,\n",
       "   'seek': 11718,\n",
       "   'start': 117.22000000000001,\n",
       "   'end': 123.02000000000001,\n",
       "   'text': ' As you can see, this entire process only takes roughly about 20 seconds to do.',\n",
       "   'tokens': [50366,\n",
       "    1018,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    11,\n",
       "    341,\n",
       "    2302,\n",
       "    1399,\n",
       "    787,\n",
       "    2516,\n",
       "    9810,\n",
       "    466,\n",
       "    945,\n",
       "    3949,\n",
       "    281,\n",
       "    360,\n",
       "    13,\n",
       "    50656],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1620811679498936,\n",
       "   'compression_ratio': 1.7142857142857142,\n",
       "   'no_speech_prob': 0.0016568207647651434},\n",
       "  {'id': 22,\n",
       "   'seek': 11718,\n",
       "   'start': 123.02000000000001,\n",
       "   'end': 129.38,\n",
       "   'text': \" Once this is done, it's going to prompt us to give it a new name and we can go ahead and just accept what it gives us.\",\n",
       "   'tokens': [50656,\n",
       "    3443,\n",
       "    341,\n",
       "    307,\n",
       "    1096,\n",
       "    11,\n",
       "    309,\n",
       "    311,\n",
       "    516,\n",
       "    281,\n",
       "    12391,\n",
       "    505,\n",
       "    281,\n",
       "    976,\n",
       "    309,\n",
       "    257,\n",
       "    777,\n",
       "    1315,\n",
       "    293,\n",
       "    321,\n",
       "    393,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    445,\n",
       "    3241,\n",
       "    437,\n",
       "    309,\n",
       "    2709,\n",
       "    505,\n",
       "    13,\n",
       "    50974],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1620811679498936,\n",
       "   'compression_ratio': 1.7142857142857142,\n",
       "   'no_speech_prob': 0.0016568207647651434},\n",
       "  {'id': 23,\n",
       "   'seek': 11718,\n",
       "   'start': 129.38,\n",
       "   'end': 130.68,\n",
       "   'text': ' Yes, I like that.',\n",
       "   'tokens': [50974, 1079, 11, 286, 411, 300, 13, 51039],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1620811679498936,\n",
       "   'compression_ratio': 1.7142857142857142,\n",
       "   'no_speech_prob': 0.0016568207647651434},\n",
       "  {'id': 24,\n",
       "   'seek': 11718,\n",
       "   'start': 130.68,\n",
       "   'end': 136.92000000000002,\n",
       "   'text': \" So go ahead and name it essentially entity explorer and we'll see that it's happy with our choice.\",\n",
       "   'tokens': [51039,\n",
       "    407,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    1315,\n",
       "    309,\n",
       "    4476,\n",
       "    13977,\n",
       "    39680,\n",
       "    293,\n",
       "    321,\n",
       "    603,\n",
       "    536,\n",
       "    300,\n",
       "    309,\n",
       "    311,\n",
       "    2055,\n",
       "    365,\n",
       "    527,\n",
       "    3922,\n",
       "    13,\n",
       "    51351],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1620811679498936,\n",
       "   'compression_ratio': 1.7142857142857142,\n",
       "   'no_speech_prob': 0.0016568207647651434},\n",
       "  {'id': 25,\n",
       "   'seek': 11718,\n",
       "   'start': 136.92000000000002,\n",
       "   'end': 140.82,\n",
       "   'text': \" And then it's finally going to prompt for us to generate essentially an image for this.\",\n",
       "   'tokens': [51351,\n",
       "    400,\n",
       "    550,\n",
       "    309,\n",
       "    311,\n",
       "    2721,\n",
       "    516,\n",
       "    281,\n",
       "    12391,\n",
       "    337,\n",
       "    505,\n",
       "    281,\n",
       "    8460,\n",
       "    4476,\n",
       "    364,\n",
       "    3256,\n",
       "    337,\n",
       "    341,\n",
       "    13,\n",
       "    51546],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1620811679498936,\n",
       "   'compression_ratio': 1.7142857142857142,\n",
       "   'no_speech_prob': 0.0016568207647651434},\n",
       "  {'id': 26,\n",
       "   'seek': 11718,\n",
       "   'start': 140.82,\n",
       "   'end': 144.96,\n",
       "   'text': \" This will be its profile image and it's using Dolly 3 to do this.\",\n",
       "   'tokens': [51546,\n",
       "    639,\n",
       "    486,\n",
       "    312,\n",
       "    1080,\n",
       "    7964,\n",
       "    3256,\n",
       "    293,\n",
       "    309,\n",
       "    311,\n",
       "    1228,\n",
       "    1144,\n",
       "    13020,\n",
       "    805,\n",
       "    281,\n",
       "    360,\n",
       "    341,\n",
       "    13,\n",
       "    51753],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1620811679498936,\n",
       "   'compression_ratio': 1.7142857142857142,\n",
       "   'no_speech_prob': 0.0016568207647651434},\n",
       "  {'id': 27,\n",
       "   'seek': 14496,\n",
       "   'start': 145.0,\n",
       "   'end': 147.24,\n",
       "   'text': ' Just a moment. This is going to be done.',\n",
       "   'tokens': [50366,\n",
       "    1449,\n",
       "    257,\n",
       "    1623,\n",
       "    13,\n",
       "    639,\n",
       "    307,\n",
       "    516,\n",
       "    281,\n",
       "    312,\n",
       "    1096,\n",
       "    13,\n",
       "    50478],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14506027915261008,\n",
       "   'compression_ratio': 1.830188679245283,\n",
       "   'no_speech_prob': 0.0053811646066606045},\n",
       "  {'id': 28,\n",
       "   'seek': 14496,\n",
       "   'start': 147.24,\n",
       "   'end': 151.64000000000001,\n",
       "   'text': \" While this is happening, let's go ahead and explore what's happening on the right hand of the screen.\",\n",
       "   'tokens': [50478,\n",
       "    3987,\n",
       "    341,\n",
       "    307,\n",
       "    2737,\n",
       "    11,\n",
       "    718,\n",
       "    311,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    6839,\n",
       "    437,\n",
       "    311,\n",
       "    2737,\n",
       "    322,\n",
       "    264,\n",
       "    558,\n",
       "    1011,\n",
       "    295,\n",
       "    264,\n",
       "    2568,\n",
       "    13,\n",
       "    50698],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14506027915261008,\n",
       "   'compression_ratio': 1.830188679245283,\n",
       "   'no_speech_prob': 0.0053811646066606045},\n",
       "  {'id': 29,\n",
       "   'seek': 14496,\n",
       "   'start': 151.64000000000001,\n",
       "   'end': 156.94,\n",
       "   'text': ' On the right side of the screen, we see things like find entities in this text and all these other little prompts.',\n",
       "   'tokens': [50698,\n",
       "    1282,\n",
       "    264,\n",
       "    558,\n",
       "    1252,\n",
       "    295,\n",
       "    264,\n",
       "    2568,\n",
       "    11,\n",
       "    321,\n",
       "    536,\n",
       "    721,\n",
       "    411,\n",
       "    915,\n",
       "    16667,\n",
       "    294,\n",
       "    341,\n",
       "    2487,\n",
       "    293,\n",
       "    439,\n",
       "    613,\n",
       "    661,\n",
       "    707,\n",
       "    41095,\n",
       "    13,\n",
       "    50963],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14506027915261008,\n",
       "   'compression_ratio': 1.830188679245283,\n",
       "   'no_speech_prob': 0.0053811646066606045},\n",
       "  {'id': 30,\n",
       "   'seek': 14496,\n",
       "   'start': 156.94,\n",
       "   'end': 160.84,\n",
       "   'text': \" This is the result of the config file that's already been generated for us.\",\n",
       "   'tokens': [50963,\n",
       "    639,\n",
       "    307,\n",
       "    264,\n",
       "    1874,\n",
       "    295,\n",
       "    264,\n",
       "    6662,\n",
       "    3991,\n",
       "    300,\n",
       "    311,\n",
       "    1217,\n",
       "    668,\n",
       "    10833,\n",
       "    337,\n",
       "    505,\n",
       "    13,\n",
       "    51158],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14506027915261008,\n",
       "   'compression_ratio': 1.830188679245283,\n",
       "   'no_speech_prob': 0.0053811646066606045},\n",
       "  {'id': 31,\n",
       "   'seek': 14496,\n",
       "   'start': 160.84,\n",
       "   'end': 166.10000000000002,\n",
       "   'text': \" And this is where we can test out how this model is working and if it's doing what we want it to do.\",\n",
       "   'tokens': [51158,\n",
       "    400,\n",
       "    341,\n",
       "    307,\n",
       "    689,\n",
       "    321,\n",
       "    393,\n",
       "    1500,\n",
       "    484,\n",
       "    577,\n",
       "    341,\n",
       "    2316,\n",
       "    307,\n",
       "    1364,\n",
       "    293,\n",
       "    498,\n",
       "    309,\n",
       "    311,\n",
       "    884,\n",
       "    437,\n",
       "    321,\n",
       "    528,\n",
       "    309,\n",
       "    281,\n",
       "    360,\n",
       "    13,\n",
       "    51421],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14506027915261008,\n",
       "   'compression_ratio': 1.830188679245283,\n",
       "   'no_speech_prob': 0.0053811646066606045},\n",
       "  {'id': 32,\n",
       "   'seek': 14496,\n",
       "   'start': 166.10000000000002,\n",
       "   'end': 168.9,\n",
       "   'text': \" And we're going to go ahead and say, yes, keep that.\",\n",
       "   'tokens': [51421,\n",
       "    400,\n",
       "    321,\n",
       "    434,\n",
       "    516,\n",
       "    281,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    584,\n",
       "    11,\n",
       "    2086,\n",
       "    11,\n",
       "    1066,\n",
       "    300,\n",
       "    13,\n",
       "    51561],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14506027915261008,\n",
       "   'compression_ratio': 1.830188679245283,\n",
       "   'no_speech_prob': 0.0053811646066606045},\n",
       "  {'id': 33,\n",
       "   'seek': 14496,\n",
       "   'start': 168.9,\n",
       "   'end': 170.24,\n",
       "   'text': \" And we're happy with it.\",\n",
       "   'tokens': [51561, 400, 321, 434, 2055, 365, 309, 13, 51628],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14506027915261008,\n",
       "   'compression_ratio': 1.830188679245283,\n",
       "   'no_speech_prob': 0.0053811646066606045},\n",
       "  {'id': 34,\n",
       "   'seek': 14496,\n",
       "   'start': 170.24,\n",
       "   'end': 173.54000000000002,\n",
       "   'text': ' So now that we have our image, our title and our set of instructions,',\n",
       "   'tokens': [51628,\n",
       "    407,\n",
       "    586,\n",
       "    300,\n",
       "    321,\n",
       "    362,\n",
       "    527,\n",
       "    3256,\n",
       "    11,\n",
       "    527,\n",
       "    4876,\n",
       "    293,\n",
       "    527,\n",
       "    992,\n",
       "    295,\n",
       "    9415,\n",
       "    11,\n",
       "    51793],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14506027915261008,\n",
       "   'compression_ratio': 1.830188679245283,\n",
       "   'no_speech_prob': 0.0053811646066606045},\n",
       "  {'id': 35,\n",
       "   'seek': 17354,\n",
       "   'start': 173.54,\n",
       "   'end': 177.32,\n",
       "   'text': \" let's go ahead and take a look at the configure section.\",\n",
       "   'tokens': [50364,\n",
       "    718,\n",
       "    311,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    747,\n",
       "    257,\n",
       "    574,\n",
       "    412,\n",
       "    264,\n",
       "    22162,\n",
       "    3541,\n",
       "    13,\n",
       "    50553],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11251991160594634,\n",
       "   'compression_ratio': 1.8355704697986577,\n",
       "   'no_speech_prob': 0.0016591375460848212},\n",
       "  {'id': 36,\n",
       "   'seek': 17354,\n",
       "   'start': 177.32,\n",
       "   'end': 181.51999999999998,\n",
       "   'text': \" As you can tell, we've gone ahead and already got this all populated for us.\",\n",
       "   'tokens': [50553,\n",
       "    1018,\n",
       "    291,\n",
       "    393,\n",
       "    980,\n",
       "    11,\n",
       "    321,\n",
       "    600,\n",
       "    2780,\n",
       "    2286,\n",
       "    293,\n",
       "    1217,\n",
       "    658,\n",
       "    341,\n",
       "    439,\n",
       "    32998,\n",
       "    337,\n",
       "    505,\n",
       "    13,\n",
       "    50763],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11251991160594634,\n",
       "   'compression_ratio': 1.8355704697986577,\n",
       "   'no_speech_prob': 0.0016591375460848212},\n",
       "  {'id': 37,\n",
       "   'seek': 17354,\n",
       "   'start': 181.51999999999998,\n",
       "   'end': 184.82,\n",
       "   'text': ' This is what GPT builder was doing in the background.',\n",
       "   'tokens': [50763,\n",
       "    639,\n",
       "    307,\n",
       "    437,\n",
       "    26039,\n",
       "    51,\n",
       "    27377,\n",
       "    390,\n",
       "    884,\n",
       "    294,\n",
       "    264,\n",
       "    3678,\n",
       "    13,\n",
       "    50928],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11251991160594634,\n",
       "   'compression_ratio': 1.8355704697986577,\n",
       "   'no_speech_prob': 0.0016591375460848212},\n",
       "  {'id': 38,\n",
       "   'seek': 17354,\n",
       "   'start': 184.82,\n",
       "   'end': 189.28,\n",
       "   'text': \" As you can tell, we've got a couple of different things here, such as the name of it, which hasn't been updated yet,\",\n",
       "   'tokens': [50928,\n",
       "    1018,\n",
       "    291,\n",
       "    393,\n",
       "    980,\n",
       "    11,\n",
       "    321,\n",
       "    600,\n",
       "    658,\n",
       "    257,\n",
       "    1916,\n",
       "    295,\n",
       "    819,\n",
       "    721,\n",
       "    510,\n",
       "    11,\n",
       "    1270,\n",
       "    382,\n",
       "    264,\n",
       "    1315,\n",
       "    295,\n",
       "    309,\n",
       "    11,\n",
       "    597,\n",
       "    6132,\n",
       "    380,\n",
       "    668,\n",
       "    10588,\n",
       "    1939,\n",
       "    11,\n",
       "    51151],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11251991160594634,\n",
       "   'compression_ratio': 1.8355704697986577,\n",
       "   'no_speech_prob': 0.0016591375460848212},\n",
       "  {'id': 39,\n",
       "   'seek': 17354,\n",
       "   'start': 189.28,\n",
       "   'end': 193.44,\n",
       "   'text': ' a description of it, which we can see populated here, and a brief set of instructions,',\n",
       "   'tokens': [51151,\n",
       "    257,\n",
       "    3855,\n",
       "    295,\n",
       "    309,\n",
       "    11,\n",
       "    597,\n",
       "    321,\n",
       "    393,\n",
       "    536,\n",
       "    32998,\n",
       "    510,\n",
       "    11,\n",
       "    293,\n",
       "    257,\n",
       "    5353,\n",
       "    992,\n",
       "    295,\n",
       "    9415,\n",
       "    11,\n",
       "    51359],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11251991160594634,\n",
       "   'compression_ratio': 1.8355704697986577,\n",
       "   'no_speech_prob': 0.0016591375460848212},\n",
       "  {'id': 40,\n",
       "   'seek': 17354,\n",
       "   'start': 193.44,\n",
       "   'end': 197.68,\n",
       "   'text': \" which it's translated for us into probably a better collection of instructions.\",\n",
       "   'tokens': [51359,\n",
       "    597,\n",
       "    309,\n",
       "    311,\n",
       "    16805,\n",
       "    337,\n",
       "    505,\n",
       "    666,\n",
       "    1391,\n",
       "    257,\n",
       "    1101,\n",
       "    5765,\n",
       "    295,\n",
       "    9415,\n",
       "    13,\n",
       "    51571],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11251991160594634,\n",
       "   'compression_ratio': 1.8355704697986577,\n",
       "   'no_speech_prob': 0.0016591375460848212},\n",
       "  {'id': 41,\n",
       "   'seek': 17354,\n",
       "   'start': 197.68,\n",
       "   'end': 202.34,\n",
       "   'text': \" It's also given us a couple conversation starters, which pop up right here.\",\n",
       "   'tokens': [51571,\n",
       "    467,\n",
       "    311,\n",
       "    611,\n",
       "    2212,\n",
       "    505,\n",
       "    257,\n",
       "    1916,\n",
       "    3761,\n",
       "    35131,\n",
       "    11,\n",
       "    597,\n",
       "    1665,\n",
       "    493,\n",
       "    558,\n",
       "    510,\n",
       "    13,\n",
       "    51804],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11251991160594634,\n",
       "   'compression_ratio': 1.8355704697986577,\n",
       "   'no_speech_prob': 0.0016591375460848212},\n",
       "  {'id': 42,\n",
       "   'seek': 20234,\n",
       "   'start': 202.34,\n",
       "   'end': 206.02,\n",
       "   'text': \" Let's go ahead and go back to create and take a look at what we can do.\",\n",
       "   'tokens': [50364,\n",
       "    961,\n",
       "    311,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    352,\n",
       "    646,\n",
       "    281,\n",
       "    1884,\n",
       "    293,\n",
       "    747,\n",
       "    257,\n",
       "    574,\n",
       "    412,\n",
       "    437,\n",
       "    321,\n",
       "    393,\n",
       "    360,\n",
       "    13,\n",
       "    50548],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09579849243164062,\n",
       "   'compression_ratio': 1.7190332326283988,\n",
       "   'no_speech_prob': 0.000700183620210737},\n",
       "  {'id': 43,\n",
       "   'seek': 20234,\n",
       "   'start': 206.02,\n",
       "   'end': 209.12,\n",
       "   'text': \" So let's go ahead and say, find entities in this text.\",\n",
       "   'tokens': [50548,\n",
       "    407,\n",
       "    718,\n",
       "    311,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    584,\n",
       "    11,\n",
       "    915,\n",
       "    16667,\n",
       "    294,\n",
       "    341,\n",
       "    2487,\n",
       "    13,\n",
       "    50703],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09579849243164062,\n",
       "   'compression_ratio': 1.7190332326283988,\n",
       "   'no_speech_prob': 0.000700183620210737},\n",
       "  {'id': 44,\n",
       "   'seek': 20234,\n",
       "   'start': 209.12,\n",
       "   'end': 217.08,\n",
       "   'text': \" But I'm just going to go ahead and paste in an opening bit of dialogue from a United States Holocaust Memorial Museum testimony.\",\n",
       "   'tokens': [50703,\n",
       "    583,\n",
       "    286,\n",
       "    478,\n",
       "    445,\n",
       "    516,\n",
       "    281,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    9163,\n",
       "    294,\n",
       "    364,\n",
       "    5193,\n",
       "    857,\n",
       "    295,\n",
       "    10221,\n",
       "    490,\n",
       "    257,\n",
       "    2824,\n",
       "    3040,\n",
       "    28399,\n",
       "    24957,\n",
       "    10967,\n",
       "    15634,\n",
       "    13,\n",
       "    51101],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09579849243164062,\n",
       "   'compression_ratio': 1.7190332326283988,\n",
       "   'no_speech_prob': 0.000700183620210737},\n",
       "  {'id': 45,\n",
       "   'seek': 20234,\n",
       "   'start': 217.08,\n",
       "   'end': 222.84,\n",
       "   'text': \" And as you can tell, all I've had to do in this is in real time is simply paste in a single block of text.\",\n",
       "   'tokens': [51101,\n",
       "    400,\n",
       "    382,\n",
       "    291,\n",
       "    393,\n",
       "    980,\n",
       "    11,\n",
       "    439,\n",
       "    286,\n",
       "    600,\n",
       "    632,\n",
       "    281,\n",
       "    360,\n",
       "    294,\n",
       "    341,\n",
       "    307,\n",
       "    294,\n",
       "    957,\n",
       "    565,\n",
       "    307,\n",
       "    2935,\n",
       "    9163,\n",
       "    294,\n",
       "    257,\n",
       "    2167,\n",
       "    3461,\n",
       "    295,\n",
       "    2487,\n",
       "    13,\n",
       "    51389],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09579849243164062,\n",
       "   'compression_ratio': 1.7190332326283988,\n",
       "   'no_speech_prob': 0.000700183620210737},\n",
       "  {'id': 46,\n",
       "   'seek': 20234,\n",
       "   'start': 222.84,\n",
       "   'end': 227.58,\n",
       "   'text': \" Now, what I like to do when I'm creating one of these builders is I like to give it as few instructions as possible,\",\n",
       "   'tokens': [51389,\n",
       "    823,\n",
       "    11,\n",
       "    437,\n",
       "    286,\n",
       "    411,\n",
       "    281,\n",
       "    360,\n",
       "    562,\n",
       "    286,\n",
       "    478,\n",
       "    4084,\n",
       "    472,\n",
       "    295,\n",
       "    613,\n",
       "    36281,\n",
       "    307,\n",
       "    286,\n",
       "    411,\n",
       "    281,\n",
       "    976,\n",
       "    309,\n",
       "    382,\n",
       "    1326,\n",
       "    9415,\n",
       "    382,\n",
       "    1944,\n",
       "    11,\n",
       "    51626],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09579849243164062,\n",
       "   'compression_ratio': 1.7190332326283988,\n",
       "   'no_speech_prob': 0.000700183620210737},\n",
       "  {'id': 47,\n",
       "   'seek': 20234,\n",
       "   'start': 227.58,\n",
       "   'end': 231.62,\n",
       "   'text': \" especially if I'm doing something like classification, which named entity recognition is.\",\n",
       "   'tokens': [51626,\n",
       "    2318,\n",
       "    498,\n",
       "    286,\n",
       "    478,\n",
       "    884,\n",
       "    746,\n",
       "    411,\n",
       "    21538,\n",
       "    11,\n",
       "    597,\n",
       "    4926,\n",
       "    13977,\n",
       "    11150,\n",
       "    307,\n",
       "    13,\n",
       "    51828],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09579849243164062,\n",
       "   'compression_ratio': 1.7190332326283988,\n",
       "   'no_speech_prob': 0.000700183620210737},\n",
       "  {'id': 48,\n",
       "   'seek': 23162,\n",
       "   'start': 231.62,\n",
       "   'end': 234.6,\n",
       "   'text': ' And the reason for this is because it will come up with a schema.',\n",
       "   'tokens': [50364,\n",
       "    400,\n",
       "    264,\n",
       "    1778,\n",
       "    337,\n",
       "    341,\n",
       "    307,\n",
       "    570,\n",
       "    309,\n",
       "    486,\n",
       "    808,\n",
       "    493,\n",
       "    365,\n",
       "    257,\n",
       "    34078,\n",
       "    13,\n",
       "    50513],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13096196886519312,\n",
       "   'compression_ratio': 1.7380191693290734,\n",
       "   'no_speech_prob': 0.0010416189907118678},\n",
       "  {'id': 49,\n",
       "   'seek': 23162,\n",
       "   'start': 234.6,\n",
       "   'end': 238.9,\n",
       "   'text': ' And I can look at the schema and say, hmm, do I like this or do I not like this?',\n",
       "   'tokens': [50513,\n",
       "    400,\n",
       "    286,\n",
       "    393,\n",
       "    574,\n",
       "    412,\n",
       "    264,\n",
       "    34078,\n",
       "    293,\n",
       "    584,\n",
       "    11,\n",
       "    16478,\n",
       "    11,\n",
       "    360,\n",
       "    286,\n",
       "    411,\n",
       "    341,\n",
       "    420,\n",
       "    360,\n",
       "    286,\n",
       "    406,\n",
       "    411,\n",
       "    341,\n",
       "    30,\n",
       "    50728],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13096196886519312,\n",
       "   'compression_ratio': 1.7380191693290734,\n",
       "   'no_speech_prob': 0.0010416189907118678},\n",
       "  {'id': 50,\n",
       "   'seek': 23162,\n",
       "   'start': 238.9,\n",
       "   'end': 244.46,\n",
       "   'text': ' And what I can do for my next iteration of this creation, when I start to adjust these instructions,',\n",
       "   'tokens': [50728,\n",
       "    400,\n",
       "    437,\n",
       "    286,\n",
       "    393,\n",
       "    360,\n",
       "    337,\n",
       "    452,\n",
       "    958,\n",
       "    24784,\n",
       "    295,\n",
       "    341,\n",
       "    8016,\n",
       "    11,\n",
       "    562,\n",
       "    286,\n",
       "    722,\n",
       "    281,\n",
       "    4369,\n",
       "    613,\n",
       "    9415,\n",
       "    11,\n",
       "    51006],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13096196886519312,\n",
       "   'compression_ratio': 1.7380191693290734,\n",
       "   'no_speech_prob': 0.0010416189907118678},\n",
       "  {'id': 51,\n",
       "   'seek': 23162,\n",
       "   'start': 244.46,\n",
       "   'end': 250.4,\n",
       "   'text': \" is I can use the schema that it's generated and go ahead and accept that and apply it into the main text.\",\n",
       "   'tokens': [51006,\n",
       "    307,\n",
       "    286,\n",
       "    393,\n",
       "    764,\n",
       "    264,\n",
       "    34078,\n",
       "    300,\n",
       "    309,\n",
       "    311,\n",
       "    10833,\n",
       "    293,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    3241,\n",
       "    300,\n",
       "    293,\n",
       "    3079,\n",
       "    309,\n",
       "    666,\n",
       "    264,\n",
       "    2135,\n",
       "    2487,\n",
       "    13,\n",
       "    51303],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13096196886519312,\n",
       "   'compression_ratio': 1.7380191693290734,\n",
       "   'no_speech_prob': 0.0010416189907118678},\n",
       "  {'id': 52,\n",
       "   'seek': 23162,\n",
       "   'start': 250.4,\n",
       "   'end': 255.1,\n",
       "   'text': \" So as you can see, we're getting a couple outputs and we have different kinds of entities being populated.\",\n",
       "   'tokens': [51303,\n",
       "    407,\n",
       "    382,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    11,\n",
       "    321,\n",
       "    434,\n",
       "    1242,\n",
       "    257,\n",
       "    1916,\n",
       "    23930,\n",
       "    293,\n",
       "    321,\n",
       "    362,\n",
       "    819,\n",
       "    3685,\n",
       "    295,\n",
       "    16667,\n",
       "    885,\n",
       "    32998,\n",
       "    13,\n",
       "    51538],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13096196886519312,\n",
       "   'compression_ratio': 1.7380191693290734,\n",
       "   'no_speech_prob': 0.0010416189907118678},\n",
       "  {'id': 53,\n",
       "   'seek': 23162,\n",
       "   'start': 255.1,\n",
       "   'end': 260.3,\n",
       "   'text': ' We have a person in this case, Agnes Adaki, and it keeps on going on down the list.',\n",
       "   'tokens': [51538,\n",
       "    492,\n",
       "    362,\n",
       "    257,\n",
       "    954,\n",
       "    294,\n",
       "    341,\n",
       "    1389,\n",
       "    11,\n",
       "    2725,\n",
       "    4081,\n",
       "    1999,\n",
       "    7421,\n",
       "    11,\n",
       "    293,\n",
       "    309,\n",
       "    5965,\n",
       "    322,\n",
       "    516,\n",
       "    322,\n",
       "    760,\n",
       "    264,\n",
       "    1329,\n",
       "    13,\n",
       "    51798],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13096196886519312,\n",
       "   'compression_ratio': 1.7380191693290734,\n",
       "   'no_speech_prob': 0.0010416189907118678},\n",
       "  {'id': 54,\n",
       "   'seek': 26030,\n",
       "   'start': 260.38,\n",
       "   'end': 264.28000000000003,\n",
       "   'text': \" And it even has extracted this individual's maiden name as well.\",\n",
       "   'tokens': [50368,\n",
       "    400,\n",
       "    309,\n",
       "    754,\n",
       "    575,\n",
       "    34086,\n",
       "    341,\n",
       "    2609,\n",
       "    311,\n",
       "    48515,\n",
       "    1315,\n",
       "    382,\n",
       "    731,\n",
       "    13,\n",
       "    50563],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1366295405796596,\n",
       "   'compression_ratio': 1.697749196141479,\n",
       "   'no_speech_prob': 0.002757254522293806},\n",
       "  {'id': 55,\n",
       "   'seek': 26030,\n",
       "   'start': 264.28000000000003,\n",
       "   'end': 267.14,\n",
       "   'text': ' If we keep on going down, we see other kinds of events.',\n",
       "   'tokens': [50563,\n",
       "    759,\n",
       "    321,\n",
       "    1066,\n",
       "    322,\n",
       "    516,\n",
       "    760,\n",
       "    11,\n",
       "    321,\n",
       "    536,\n",
       "    661,\n",
       "    3685,\n",
       "    295,\n",
       "    3931,\n",
       "    13,\n",
       "    50706],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1366295405796596,\n",
       "   'compression_ratio': 1.697749196141479,\n",
       "   'no_speech_prob': 0.002757254522293806},\n",
       "  {'id': 56,\n",
       "   'seek': 26030,\n",
       "   'start': 267.14,\n",
       "   'end': 269.44,\n",
       "   'text': ' In this case, this is a place.',\n",
       "   'tokens': [50706, 682, 341, 1389, 11, 341, 307, 257, 1081, 13, 50821],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1366295405796596,\n",
       "   'compression_ratio': 1.697749196141479,\n",
       "   'no_speech_prob': 0.002757254522293806},\n",
       "  {'id': 57,\n",
       "   'seek': 26030,\n",
       "   'start': 269.44,\n",
       "   'end': 275.78000000000003,\n",
       "   'text': \" And it's also specifically referencing Budapest Hungry and the date of the event actually taking place.\",\n",
       "   'tokens': [50821,\n",
       "    400,\n",
       "    309,\n",
       "    311,\n",
       "    611,\n",
       "    4682,\n",
       "    40582,\n",
       "    6384,\n",
       "    569,\n",
       "    377,\n",
       "    15063,\n",
       "    627,\n",
       "    293,\n",
       "    264,\n",
       "    4002,\n",
       "    295,\n",
       "    264,\n",
       "    2280,\n",
       "    767,\n",
       "    1940,\n",
       "    1081,\n",
       "    13,\n",
       "    51138],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1366295405796596,\n",
       "   'compression_ratio': 1.697749196141479,\n",
       "   'no_speech_prob': 0.002757254522293806},\n",
       "  {'id': 58,\n",
       "   'seek': 26030,\n",
       "   'start': 275.78000000000003,\n",
       "   'end': 278.98,\n",
       "   'text': \" So in this case, it isn't functioning like a typical NER model.\",\n",
       "   'tokens': [51138,\n",
       "    407,\n",
       "    294,\n",
       "    341,\n",
       "    1389,\n",
       "    11,\n",
       "    309,\n",
       "    1943,\n",
       "    380,\n",
       "    18483,\n",
       "    411,\n",
       "    257,\n",
       "    7476,\n",
       "    426,\n",
       "    1598,\n",
       "    2316,\n",
       "    13,\n",
       "    51298],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1366295405796596,\n",
       "   'compression_ratio': 1.697749196141479,\n",
       "   'no_speech_prob': 0.002757254522293806},\n",
       "  {'id': 59,\n",
       "   'seek': 26030,\n",
       "   'start': 278.98,\n",
       "   'end': 281.38,\n",
       "   'text': \" Rather, it's giving a lot of extra metadata.\",\n",
       "   'tokens': [51298,\n",
       "    16571,\n",
       "    11,\n",
       "    309,\n",
       "    311,\n",
       "    2902,\n",
       "    257,\n",
       "    688,\n",
       "    295,\n",
       "    2857,\n",
       "    26603,\n",
       "    13,\n",
       "    51418],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1366295405796596,\n",
       "   'compression_ratio': 1.697749196141479,\n",
       "   'no_speech_prob': 0.002757254522293806},\n",
       "  {'id': 60,\n",
       "   'seek': 26030,\n",
       "   'start': 281.38,\n",
       "   'end': 285.88,\n",
       "   'text': \" What's really nice about this is for as far as I can tell, I'm not seeing any hallucinations.\",\n",
       "   'tokens': [51418,\n",
       "    708,\n",
       "    311,\n",
       "    534,\n",
       "    1481,\n",
       "    466,\n",
       "    341,\n",
       "    307,\n",
       "    337,\n",
       "    382,\n",
       "    1400,\n",
       "    382,\n",
       "    286,\n",
       "    393,\n",
       "    980,\n",
       "    11,\n",
       "    286,\n",
       "    478,\n",
       "    406,\n",
       "    2577,\n",
       "    604,\n",
       "    35212,\n",
       "    10325,\n",
       "    13,\n",
       "    51643],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1366295405796596,\n",
       "   'compression_ratio': 1.697749196141479,\n",
       "   'no_speech_prob': 0.002757254522293806},\n",
       "  {'id': 61,\n",
       "   'seek': 26030,\n",
       "   'start': 285.88,\n",
       "   'end': 289.2,\n",
       "   'text': \" Now, that's not to indicate that hallucinations won't actually occur.\",\n",
       "   'tokens': [51643,\n",
       "    823,\n",
       "    11,\n",
       "    300,\n",
       "    311,\n",
       "    406,\n",
       "    281,\n",
       "    13330,\n",
       "    300,\n",
       "    35212,\n",
       "    10325,\n",
       "    1582,\n",
       "    380,\n",
       "    767,\n",
       "    5160,\n",
       "    13,\n",
       "    51809],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1366295405796596,\n",
       "   'compression_ratio': 1.697749196141479,\n",
       "   'no_speech_prob': 0.002757254522293806},\n",
       "  {'id': 62,\n",
       "   'seek': 28920,\n",
       "   'start': 289.2,\n",
       "   'end': 291.59999999999997,\n",
       "   'text': ' In fact, you can probably expect that they do.',\n",
       "   'tokens': [50364,\n",
       "    682,\n",
       "    1186,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    1391,\n",
       "    2066,\n",
       "    300,\n",
       "    436,\n",
       "    360,\n",
       "    13,\n",
       "    50484],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10970701402233493,\n",
       "   'compression_ratio': 1.755952380952381,\n",
       "   'no_speech_prob': 0.011250898241996765},\n",
       "  {'id': 63,\n",
       "   'seek': 28920,\n",
       "   'start': 291.59999999999997,\n",
       "   'end': 297.4,\n",
       "   'text': ' What I like to do though, is just use this as a way to kind of explore my data real quickly and do some manual validation.',\n",
       "   'tokens': [50484,\n",
       "    708,\n",
       "    286,\n",
       "    411,\n",
       "    281,\n",
       "    360,\n",
       "    1673,\n",
       "    11,\n",
       "    307,\n",
       "    445,\n",
       "    764,\n",
       "    341,\n",
       "    382,\n",
       "    257,\n",
       "    636,\n",
       "    281,\n",
       "    733,\n",
       "    295,\n",
       "    6839,\n",
       "    452,\n",
       "    1412,\n",
       "    957,\n",
       "    2661,\n",
       "    293,\n",
       "    360,\n",
       "    512,\n",
       "    9688,\n",
       "    24071,\n",
       "    13,\n",
       "    50774],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10970701402233493,\n",
       "   'compression_ratio': 1.755952380952381,\n",
       "   'no_speech_prob': 0.011250898241996765},\n",
       "  {'id': 64,\n",
       "   'seek': 28920,\n",
       "   'start': 297.4,\n",
       "   'end': 301.74,\n",
       "   'text': ' This helps me think through ways in which I can kind of structure structured metadata.',\n",
       "   'tokens': [50774,\n",
       "    639,\n",
       "    3665,\n",
       "    385,\n",
       "    519,\n",
       "    807,\n",
       "    2098,\n",
       "    294,\n",
       "    597,\n",
       "    286,\n",
       "    393,\n",
       "    733,\n",
       "    295,\n",
       "    3877,\n",
       "    18519,\n",
       "    26603,\n",
       "    13,\n",
       "    50991],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10970701402233493,\n",
       "   'compression_ratio': 1.755952380952381,\n",
       "   'no_speech_prob': 0.011250898241996765},\n",
       "  {'id': 65,\n",
       "   'seek': 28920,\n",
       "   'start': 301.74,\n",
       "   'end': 305.09999999999997,\n",
       "   'text': \" But let's just pretend for just a moment that I'd like all of this.\",\n",
       "   'tokens': [50991,\n",
       "    583,\n",
       "    718,\n",
       "    311,\n",
       "    445,\n",
       "    11865,\n",
       "    337,\n",
       "    445,\n",
       "    257,\n",
       "    1623,\n",
       "    300,\n",
       "    286,\n",
       "    1116,\n",
       "    411,\n",
       "    439,\n",
       "    295,\n",
       "    341,\n",
       "    13,\n",
       "    51159],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10970701402233493,\n",
       "   'compression_ratio': 1.755952380952381,\n",
       "   'no_speech_prob': 0.011250898241996765},\n",
       "  {'id': 66,\n",
       "   'seek': 28920,\n",
       "   'start': 305.09999999999997,\n",
       "   'end': 306.64,\n",
       "   'text': ' What can I do at this stage?',\n",
       "   'tokens': [51159, 708, 393, 286, 360, 412, 341, 3233, 30, 51236],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10970701402233493,\n",
       "   'compression_ratio': 1.755952380952381,\n",
       "   'no_speech_prob': 0.011250898241996765},\n",
       "  {'id': 67,\n",
       "   'seek': 28920,\n",
       "   'start': 306.64,\n",
       "   'end': 309.88,\n",
       "   'text': \" Well, let's say I want to just work with this internally.\",\n",
       "   'tokens': [51236,\n",
       "    1042,\n",
       "    11,\n",
       "    718,\n",
       "    311,\n",
       "    584,\n",
       "    286,\n",
       "    528,\n",
       "    281,\n",
       "    445,\n",
       "    589,\n",
       "    365,\n",
       "    341,\n",
       "    19501,\n",
       "    13,\n",
       "    51398],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10970701402233493,\n",
       "   'compression_ratio': 1.755952380952381,\n",
       "   'no_speech_prob': 0.011250898241996765},\n",
       "  {'id': 68,\n",
       "   'seek': 28920,\n",
       "   'start': 309.88,\n",
       "   'end': 315.5,\n",
       "   'text': ' I can click the Save button and I can select I only want to be the one to view this and work with it.',\n",
       "   'tokens': [51398,\n",
       "    286,\n",
       "    393,\n",
       "    2052,\n",
       "    264,\n",
       "    15541,\n",
       "    2960,\n",
       "    293,\n",
       "    286,\n",
       "    393,\n",
       "    3048,\n",
       "    286,\n",
       "    787,\n",
       "    528,\n",
       "    281,\n",
       "    312,\n",
       "    264,\n",
       "    472,\n",
       "    281,\n",
       "    1910,\n",
       "    341,\n",
       "    293,\n",
       "    589,\n",
       "    365,\n",
       "    309,\n",
       "    13,\n",
       "    51679],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10970701402233493,\n",
       "   'compression_ratio': 1.755952380952381,\n",
       "   'no_speech_prob': 0.011250898241996765},\n",
       "  {'id': 69,\n",
       "   'seek': 28920,\n",
       "   'start': 315.5,\n",
       "   'end': 318.88,\n",
       "   'text': \" I'll do this when I'm developing this, at least testing out a couple things.\",\n",
       "   'tokens': [51679,\n",
       "    286,\n",
       "    603,\n",
       "    360,\n",
       "    341,\n",
       "    562,\n",
       "    286,\n",
       "    478,\n",
       "    6416,\n",
       "    341,\n",
       "    11,\n",
       "    412,\n",
       "    1935,\n",
       "    4997,\n",
       "    484,\n",
       "    257,\n",
       "    1916,\n",
       "    721,\n",
       "    13,\n",
       "    51848],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10970701402233493,\n",
       "   'compression_ratio': 1.755952380952381,\n",
       "   'no_speech_prob': 0.011250898241996765},\n",
       "  {'id': 70,\n",
       "   'seek': 31888,\n",
       "   'start': 318.92,\n",
       "   'end': 326.12,\n",
       "   'text': \" When I'm ready to send this out and have other individuals test this model and how it's performing on out of scope data that probably I can't come up with,\",\n",
       "   'tokens': [50366,\n",
       "    1133,\n",
       "    286,\n",
       "    478,\n",
       "    1919,\n",
       "    281,\n",
       "    2845,\n",
       "    341,\n",
       "    484,\n",
       "    293,\n",
       "    362,\n",
       "    661,\n",
       "    5346,\n",
       "    1500,\n",
       "    341,\n",
       "    2316,\n",
       "    293,\n",
       "    577,\n",
       "    309,\n",
       "    311,\n",
       "    10205,\n",
       "    322,\n",
       "    484,\n",
       "    295,\n",
       "    11923,\n",
       "    1412,\n",
       "    300,\n",
       "    1391,\n",
       "    286,\n",
       "    393,\n",
       "    380,\n",
       "    808,\n",
       "    493,\n",
       "    365,\n",
       "    11,\n",
       "    50726],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14463390244377983,\n",
       "   'compression_ratio': 1.6583629893238434,\n",
       "   'no_speech_prob': 0.002557853702455759},\n",
       "  {'id': 71,\n",
       "   'seek': 31888,\n",
       "   'start': 326.12,\n",
       "   'end': 331.12,\n",
       "   'text': ' I can then allow only people with the link to actually access and use this model.',\n",
       "   'tokens': [50726,\n",
       "    286,\n",
       "    393,\n",
       "    550,\n",
       "    2089,\n",
       "    787,\n",
       "    561,\n",
       "    365,\n",
       "    264,\n",
       "    2113,\n",
       "    281,\n",
       "    767,\n",
       "    2105,\n",
       "    293,\n",
       "    764,\n",
       "    341,\n",
       "    2316,\n",
       "    13,\n",
       "    50976],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14463390244377983,\n",
       "   'compression_ratio': 1.6583629893238434,\n",
       "   'no_speech_prob': 0.002557853702455759},\n",
       "  {'id': 72,\n",
       "   'seek': 31888,\n",
       "   'start': 331.12,\n",
       "   'end': 339.21999999999997,\n",
       "   'text': ' And then finally, once I pass that test, I can go ahead and select Public and make this publicly available for others to test it and give larger feedback.',\n",
       "   'tokens': [50976,\n",
       "    400,\n",
       "    550,\n",
       "    2721,\n",
       "    11,\n",
       "    1564,\n",
       "    286,\n",
       "    1320,\n",
       "    300,\n",
       "    1500,\n",
       "    11,\n",
       "    286,\n",
       "    393,\n",
       "    352,\n",
       "    2286,\n",
       "    293,\n",
       "    3048,\n",
       "    9489,\n",
       "    293,\n",
       "    652,\n",
       "    341,\n",
       "    14843,\n",
       "    2435,\n",
       "    337,\n",
       "    2357,\n",
       "    281,\n",
       "    1500,\n",
       "    309,\n",
       "    293,\n",
       "    976,\n",
       "    4833,\n",
       "    5824,\n",
       "    13,\n",
       "    51381],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14463390244377983,\n",
       "   'compression_ratio': 1.6583629893238434,\n",
       "   'no_speech_prob': 0.002557853702455759},\n",
       "  {'id': 73,\n",
       "   'seek': 31888,\n",
       "   'start': 339.21999999999997,\n",
       "   'end': 343.62,\n",
       "   'text': \" But overall, that's a very quick introduction to this new open AI feature\",\n",
       "   'tokens': [51381,\n",
       "    583,\n",
       "    4787,\n",
       "    11,\n",
       "    300,\n",
       "    311,\n",
       "    257,\n",
       "    588,\n",
       "    1702,\n",
       "    9339,\n",
       "    281,\n",
       "    341,\n",
       "    777,\n",
       "    1269,\n",
       "    7318,\n",
       "    4111,\n",
       "    51601],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14463390244377983,\n",
       "   'compression_ratio': 1.6583629893238434,\n",
       "   'no_speech_prob': 0.002557853702455759}],\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holocaust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
